{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Neural Network from Scratch using Numpy\"\n",
    "> \"We will replicate pytorch functionalities using only Numpy\"\n",
    "\n",
    "- author: Bhargav Lad\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- image: images/nn_numpy.gif\n",
    "- categories: [ jupyter,neural-network,numpy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Class classification\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "5. class:\n",
    "```\n",
    " 1. Iris Setosa\n",
    " 2. Iris Versicolour\n",
    " 3. Iris Virginica\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries used\n",
    "\n",
    "- `sklearn.datasets.load_iris`: To load iris dataset. Iris dataset comes built into Sklearn\n",
    "- `sklearn.model_selection.train_test_split` : To split the data into train and test set\n",
    "- `numpy`: Use numpy arrays to create neural network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset\n",
    "\n",
    "we will load Iris dataset from sklearn's built in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1]]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y =  datasets.load_iris(return_X_y=True,as_frame=False)\n",
    "X[:10],y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test split\n",
    "\n",
    "We will split the data into 70:30 ratio where we will use 70% of data for training our model and rest 30% for evaluating our model afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,test_X, train_y,test_y = train_test_split(X,y,test_size=0.3,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network  Layers\n",
    "\n",
    "We define the layers which are required by our model. Each layer has a forward pass and backward pass.In forward pass we will compute the output of the network given the input data and in the backward pass we will compute the gradient for the layer with respect to loss.\n",
    "\n",
    "## Linear Layer\n",
    "\n",
    "**Forward Pass**\n",
    "$$\n",
    "L=X.W^{T}+b_{1}\n",
    "$$\n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "Gradient for W.r.t W:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W}=\\frac{\\partial L}{\\partial Y}^{T} . X \n",
    "$$\n",
    "Gradient for W.r.t b:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_{1}}=\\frac{\\partial L}{\\partial Y}^{T}\n",
    "$$\n",
    "Gradient for W.r.t X:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X}=\\frac{\\partial L}{\\partial Y} . W \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self,in_features,out_features,bias=True):\n",
    "        self.weight = {\"value\":None,\"grad\":None}\n",
    "        self.bias = {\"value\":None,\"grad\":None}\n",
    "        self.inp = None\n",
    "\n",
    "        self.weight[\"value\"] = np.random.uniform(-1,1,(out_features,in_features))\n",
    "        self.weight[\"grad\"] = np.random.uniform(-1,1,(out_features,in_features))\n",
    "\n",
    "        if bias==True:\n",
    "            self.bias[\"value\"] = np.random.uniform(-1,1,(out_features))\n",
    "            self.bias[\"grad\"] = np.random.uniform(-1,1,(out_features))\n",
    "        else:\n",
    "            self.bias[\"value\"] = np.zeros((out_features))\n",
    "            self.bias[\"grad\"] = np.zeros((out_features))\n",
    "\n",
    "\n",
    "    def forward(self,inp):\n",
    "        \"\"\"\n",
    "        inp: [#_samples,in_fetures]\n",
    "        \"\"\"\n",
    "        self.inp = inp\n",
    "        return np.dot(self.inp,self.weight[\"value\"].T)+self.bias[\"value\"]\n",
    "\n",
    "    def backward(self,grad):\n",
    "        \"\"\"\n",
    "        grad : shape(#,out_feat) [same as the return of forward]\n",
    "        \"\"\"\n",
    "\n",
    "        self.weight[\"grad\"] = np.dot(grad.T,self.inp )  # (out,in) = (out,#)(#,in)\n",
    "        self.bias[\"grad\"] = grad.sum(0)                 # (out) = (out)\n",
    "\n",
    "        inp_grad = np.dot(grad, self.weight[\"value\"])   # (#,in) = (#,out)(out,in)\n",
    "\n",
    "        return inp_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relu Activation\n",
    "\n",
    "**forward pass**\n",
    "\n",
    "$$\n",
    "R(x)=\\max (0, x)\n",
    "$$\n",
    "\n",
    "**backward pass**\n",
    "\n",
    "Gradient for W.r.t X:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X}=R(X) \\frac{\\partial L}{\\partial Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "\n",
    "    def forward(self,inp):\n",
    "        self.inp = inp\n",
    "        return np.clip(self.inp,0.,np.inf)\n",
    "    \n",
    "    def backward(self,grad):\n",
    "        \"\"\"\n",
    "        grad : [same as the inp]\n",
    "        \"\"\"\n",
    "        inp_grad = (self.inp>0)*grad\n",
    "        return inp_grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "We will use Cross Entropy as our loss function as we are dealing with multi-class classification problem\n",
    "\n",
    "$$\n",
    "L=-\\frac{1}{m} \\sum_{i=1}^{m} y_{i} \\cdot \\log \\left(\\hat{y}_{i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def __init__(self):\n",
    "        pass;\n",
    "\n",
    "    @staticmethod\n",
    "    def Softmax(x):\n",
    "        exps = np.exp(x)\n",
    "        return exps/exps.sum(1,keepdims=True)\n",
    "\n",
    "    def forward(self,pred,target):\n",
    "        \"\"\"\n",
    "        pred: (#,num_target)\n",
    "        target: (#,1)\n",
    "        \"\"\"\n",
    "        m = target.shape[0]\n",
    "        # take softmax along col\n",
    "        self.pred = self.Softmax(pred)\n",
    "        self.target = target\n",
    "\n",
    "        log_likelihood = -np.log(self.pred[range(m),self.target])\n",
    "        loss = log_likelihood.sum() / m\n",
    "        return loss\n",
    "\n",
    "    def backward(self,grad=1):\n",
    "        \"\"\"\n",
    "        grad : [same as the inp]\n",
    "        \"\"\"\n",
    "        m = self.target.shape[0]\n",
    "        # take softmax along col\n",
    "        self.grad = self.pred\n",
    "        self.grad[range(m),self.target] -= grad;\n",
    "        self.grad = self.grad/m\n",
    "        return self.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class\n",
    "\n",
    "We will also create a model class that will wrap our training and testing logic. This is so that we have a clean api to work with inside our training loop\n",
    "\n",
    "`fit` : To train the network\n",
    "\n",
    "`evaluate`: To evaulate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,model,loss_func,learning_rate):\n",
    "        self.model = model\n",
    "        self.loss = loss_func\n",
    "        self.lr = learning_rate  \n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        \n",
    "        # forward pass \n",
    "        pred_logits = reduce(lambda acc,curr: curr.forward(acc),self.model,x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = self.loss.forward(pred_logits,y)\n",
    "        g = self.loss.backward()\n",
    "\n",
    "        # backward pass\n",
    "        g = reduce(lambda acc,curr: curr.backward(acc),self.model[::-1],g)\n",
    "\n",
    "        # update weights\n",
    "        for l in self.model:\n",
    "            if isinstance(l,Linear):\n",
    "                l.weight[\"value\"] -= self.lr*l.weight[\"grad\"]\n",
    "                l.bias[\"value\"] -= self.lr*l.bias[\"grad\"]\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self,x):\n",
    "        pred_logits = reduce(lambda acc,curr: curr.forward(acc),self.model,x)\n",
    "        return pred_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score function/metric\n",
    "\n",
    "We will use Accuracy as out Score function to see how well our model performed on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,target):\n",
    "    pred = CrossEntropy.Softmax(pred)\n",
    "    pred = np.argmax(pred,1)\n",
    "    return (pred==target).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "**Model**\n",
    "\n",
    "1. Linear(4,100)\n",
    "2. Relu\n",
    "3. Linear(100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 50  :2.0528926556788556, accuracy: 0.3523809523809524\n",
      "Loss at epoch 100  :0.5423031512767295, accuracy: 0.8666666666666667\n",
      "Loss at epoch 150  :0.3816058479761151, accuracy: 0.9523809523809523\n",
      "Loss at epoch 200  :0.3243114702106764, accuracy: 0.9619047619047619\n",
      "Loss at epoch 250  :0.29193951765336557, accuracy: 0.9619047619047619\n",
      "Loss at epoch 300  :0.2691038630894149, accuracy: 0.9619047619047619\n",
      "Loss at epoch 350  :0.2515483517334566, accuracy: 0.9714285714285714\n",
      "Loss at epoch 400  :0.23765863708233953, accuracy: 0.9714285714285714\n",
      "Loss at epoch 450  :0.22612454813203908, accuracy: 0.9714285714285714\n",
      "Loss at epoch 500  :0.21635705475306402, accuracy: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "model = [\n",
    "    Linear(4,100),\n",
    "    Relu(),\n",
    "    Linear(100,3)]\n",
    "\n",
    "loss_function = CrossEntropy()  # Loss function\n",
    "lr=1e-3  # Learning Rate\n",
    "my_model = Model(model,loss_function,lr)  # initialize model\n",
    "epochs = 500  # epochs to train\n",
    "\n",
    "\n",
    "for i in range(1,epochs+1):\n",
    "    \n",
    "    loss = my_model.fit(train_X,train_y) # train\n",
    "    \n",
    "    pred_logits = my_model.evaluate(train_X) # get predictions\n",
    "    \n",
    "    # print Loss and accuracy every 50 epochs\n",
    "    if i%50==0:\n",
    "        print(f\"Loss at epoch {i}  :{loss}, accuracy: {accuracy(pred_logits,train_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the test set: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "pred = my_model.evaluate(test_X)\n",
    "print(f\"Accuracy for the test set: {accuracy(pred,test_y)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
