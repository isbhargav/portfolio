{
  
    
        "post0": {
            "title": "Matplotlib Guide - p2",
            "content": "import numpy as np import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) . Line Plot . One of the simplest types of plots in Matplotlib is line, which draws a line defined by a function, such as y=f(x) . Properties of Line plot . x,y: Data points, horizontal and vertical. Typically, these parameters are 1D arrays. | fmt: A format string, e.g. ‘ro’ for red circles. We will later discuss this parameter in greater depth. fmt = &#39;[marker][line][color]&#39; | label: A string, which can be used in the legend. | linestyle: The style of the line, which can be ‘-’, ‘–’, ‘-.’, or &#39;:&#39; | linewidth: The width of the line. | marker: Marker style | color: Marker color | . Line Styles . ‘-’: solid line style| | ‘–’: dashed line style| | ‘-.’: dash-dot line style| | ‘:’: dotted line style| | Color . ‘b’ : blue | ‘g’ : green | ‘r’ : red | ‘c’ : cyan | ‘m’ : magenta | ‘y’ : yellow | ‘b’ : black | ‘w’ : white | Marker . ‘.’: point marker | ‘o’: circle marker | ‘v’: triangle_down marker | ‘^’: triangle_up marker | ‘*’: star marker | ‘s’: square marker | ‘+’: plus marker | x = np.linspace(-4,4,200) fig,ax = plt.subplots() ax.plot(x,np.sin(x)) . [&lt;matplotlib.lines.Line2D at 0x7f4053096eb8&gt;] . fig,ax = plt.subplots() ax.plot(x,np.sin(x),x,np.cos(x)) . [&lt;matplotlib.lines.Line2D at 0x7f405276cba8&gt;, &lt;matplotlib.lines.Line2D at 0x7f4052795160&gt;] . fig, axe = plt.subplots(dpi=100) axe.plot(x, x + 2, linestyle=&#39;-&#39;, color=&#39;r&#39;, marker=&#39;x&#39;, label=&quot;line1&quot;) axe.plot(x, x + 3, linestyle=&#39;-&#39;, color=&#39;c&#39;, marker=&#39;s&#39;, label=&quot;line2&quot;) axe.plot(x, x + 4, linestyle=&#39;-&#39;, color=&#39;m&#39;, marker=&#39;|&#39;, label=&quot;line3&quot;) axe.plot(x, x + 5, linestyle=&#39;--&#39;, color=&#39;b&#39;, label=&quot;line4&quot;) axe.plot(x, x + 6, linestyle=&#39;-.&#39;, color=&#39;y&#39;, label=&quot;line5&quot;) axe.plot(x, x + 7, linestyle=&#39;:&#39;, color=&#39;b&#39;, label=&quot;line6&quot;) axe.legend() . &lt;matplotlib.legend.Legend at 0x7f4052296390&gt; . Scatter Plot . A scatter plot is used to plot data points on a figure based on the horizontal and vertical axes. Scatter plots can be used to show the relationship between two variables. They can also show how data clusters in a dataset. We can use scatter to show the relationship between variables, or to show the distribution of data. . Properties of Scatter plot . x,y: Data points, horizontal and vertical. Commonly, these * parameters are 1D arrays. | s: A scalar or array, used to set the size of the marker. | c: A single color string or a sequence of colors, used to set the color of the marker. | marker: Sets the shape of the marker. We went over a marker list in the previous lesson… | cmp: Colormap is a way to map color, used only when c is an array of float. | alpha: The blending value, which we can set between 0 (transparent) and 1 (opaque). | . rng = np.random.RandomState(32) x = rng.randn(50) y = rng.randn(50) fig,ax = plt.subplots(dpi=200) colors = rng.randn(50) size = rng.randn(50)*500 # ax.grid() ax.set_xlabel(&#39;Your X label&#39;) ax.set_ylabel(&#39;Your Y label&#39;) ax.set_title(&#39;Title of Graph&#39;) ax.scatter(x=x,y=y,c=colors,s=size,alpha=0.5) . /usr/local/lib/python3.6/dist-packages/matplotlib/collections.py:885: RuntimeWarning: invalid value encountered in sqrt scale = np.sqrt(self._sizes) * dpi / 72.0 * self._factor . &lt;matplotlib.collections.PathCollection at 0x7f40521b0a20&gt; . Bar Chart . Bar chart are used to display values associated with categorical data. . Properties of Bar chart . x : Controls the sequence of scalars, or the x coordinates of the bars. | height : Controls the sequence of scalars, or the heights of the bars. In short, this parameter displays the value(s) of our data. | width : Sets the width of the bar. The default value is 0.8. | bottom : Sets the y coordinate(s) of the bases of the bars. The default value is 0. | color : Sets the color of the bar faces. | orientation : Determines the orientation of the bars, {‘vertical’, ‘horizontal’}. The default value is ‘vertical’. | . fig, ax= plt.subplots(dpi=200) label = [&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;] values = [100, 200, 300, 150, 440, 700, 350, 505] ax.bar(label, values) . &lt;BarContainer object of 8 artists&gt; . More than One bar in a chart . Many times, multiple sets of data are bound to the same variable. In these cases, we need to show the data together on the same chart for comparison. On a bar chart, we can do this by using two sets of bars. . label = [&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;] values1 = [100, 200, 300, 150, 440, 700, 350, 505] values2 = [200, 250, 360, 180, 640, 780, 520, 580] values3 = [100, 200, 300, 150, 440, 700, 350, 505] index = np.arange(len(label)) fig,ax = plt.subplots(dpi=200) ax.bar(index,values1,width=0.3) ax.bar(index+0.3,values2,width=0.3) ax.bar(index+0.6,values3,width=0.3) ax.set_xticks(index+0.3) ax.set_xticklabels(label) . [Text(0, 0, &#39;Jan&#39;), Text(0, 0, &#39;Feb&#39;), Text(0, 0, &#39;Mar&#39;), Text(0, 0, &#39;Apr&#39;), Text(0, 0, &#39;May&#39;), Text(0, 0, &#39;Jun&#39;), Text(0, 0, &#39;Jul&#39;), Text(0, 0, &#39;Aug&#39;)] . Stacked Bar Chart . Stacking the bars is a useful feature that allows us to stack multiple bars on top of each other. . label = [&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;] values1 = np.array([100, 200, 300, 150, 440, 700, 350, 505]) values2 = np.array([200, 250, 360, 180, 640, 780, 520, 580]) values3 = np.array([100, 200, 300, 150, 440, 700, 350, 505]) index = np.arange(len(label)) fig,ax = plt.subplots(dpi=100) ax.bar(index,values1) ax.bar(index,values2,bottom=values1) ax.bar(index,values3,bottom=values1+values2) ax.set_xticks(index) ax.set_xticklabels(label) . [Text(0, 0, &#39;Jan&#39;), Text(0, 0, &#39;Feb&#39;), Text(0, 0, &#39;Mar&#39;), Text(0, 0, &#39;Apr&#39;), Text(0, 0, &#39;May&#39;), Text(0, 0, &#39;Jun&#39;), Text(0, 0, &#39;Jul&#39;), Text(0, 0, &#39;Aug&#39;)] . Horizontal Bar Chart . When you have many catogaries its better to represent bar chart in horizontal format. . fig, axe = plt.subplots(dpi=100) label = [&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;] index = np.arange(len(label)) values1 = [100, 200, 300, 150, 440, 700, 350, 505] values2 = [200, 250, 360, 180, 640, 780, 520, 580] axe.barh(index, values1) axe.barh(index, values2, left=values1) axe.set_yticks(index) axe.set_yticklabels(label) . [Text(0, 0, &#39;Jan&#39;), Text(0, 0, &#39;Feb&#39;), Text(0, 0, &#39;Mar&#39;), Text(0, 0, &#39;Apr&#39;), Text(0, 0, &#39;May&#39;), Text(0, 0, &#39;Jun&#39;), Text(0, 0, &#39;Jul&#39;), Text(0, 0, &#39;Aug&#39;)] . Error Bar . Error bars are graphical representations of the variability of data. They are used to indicate the error or uncertainty in a reported measurement. Error bars give a general idea of how precise a measurement is. For example, say we want to give a prediction about car sales for the next 12 months, but we are not 100% sure about our prediction. In order to indicate this uncertainty, we would provide a relative possible error. Error bars often represent one standard deviation of uncertainty, one standard error, or a particular confidence interval (e.g., a 95% confidence interval). . Properties of Error Bar . x,y: Defines the data location. | xerr,yerr: Sets the corresponding error to x, y. | fmt: Sets the format for the data points/data lines. | ecolor: Sets the color of the error line. | elinewidth: Sets the line width of the error bar. | uplims,lolims: It can be True or False. The default value is False. These arguments can be used to indicate that value gives only upper/lower limits. | capsize: Sets the length of the error bar caps in points. | . x = np.linspace(1, 10, num=10) y = 2 * np.sin(x/20 * np.pi) yerr = np.random.normal(0, 0.3, 10) fig, axe = plt.subplots(dpi=100) axe.errorbar(x, y, yerr=yerr) axe.errorbar(x, y+1, yerr=yerr) axe.errorbar(x, y+2, yerr=yerr, fmt=&quot;-o&quot;) . &lt;ErrorbarContainer object of 3 artists&gt; . labels = [&quot;dog&quot;, &quot;fish&quot;, &quot;cat&quot;, &quot;bird&quot;, &quot;sheep&quot;, &quot;horse&quot;] values = [10, 8, 12, 7, 5, 9] yerr = [1, 2, 3, 1, 2, 3] fig, axe = plt.subplots(1,2,dpi=100,figsize=(20,10)) axe[0].bar(np.arange(0, len(labels)), values, label=labels, yerr=yerr, alpha=0.7, ecolor=&#39;r&#39;, capsize=8) axe[0].set_xticks(np.arange(0, len(labels))) axe[0].set_xticklabels(labels) axe[1].errorbar(labels,values,yerr=yerr) . &lt;ErrorbarContainer object of 3 artists&gt; . Histogram . The histogram is an important graph in statistics and data analysis. It can be used to help people quickly understand the distribution of data. In order to draw a histogram, we follow the steps outlined below: . Step 1: Bin the range of your data. Step 2: Divide the entire range of values into their corresponding bins. Step 3: Count how many values fall into each different bin . Properties of Histogram . x : Our input values, either a single list/array or multiple sequences of arrays. | bins : If bins is set with an integer, it will define the number of equal-width bins within a range. If bins is set with a sequence, it will define the bin edges, including the left edge of the first bin and the right edge of the last bin. | histtype : Sets the style of the histogram. The default value is bar. step generates a line plot that is unfilled by default. stepfilled generates a line plot that is filled by default. | density : Sets True or False. The default is set to False. If True, the histogram will be normalized to form a probability density. | cumulative : Sets True or -1. If True, then a histogram is computed where each bin gives the count in that bin plus all bins for smaller values. | . data = np.random.randn(2000) fig,ax = plt.subplots(dpi=100) ax.hist(data,bins=10) . (array([ 9., 80., 240., 487., 533., 416., 180., 47., 7., 1.]), array([-3.1873996 , -2.46062252, -1.73384544, -1.00706837, -0.28029129, 0.44648578, 1.17326286, 1.90003993, 2.62681701, 3.35359409, 4.08037116]), &lt;a list of 10 Patch objects&gt;) . fig, axe = plt.subplots(nrows=2, ncols=2, dpi=200) plt.tight_layout() axe[0][0].hist(data, bins=30) axe[0][0].set_title(&quot;set bins=30&quot;) axe[0][1].hist(data, density=True) axe[0][1].set_title(&quot;normalized&quot;) axe[1][0].hist(data, color=&quot;r&quot;) axe[1][0].set_title(&quot;set color as red&quot;) axe[1][1].hist(data, histtype=&#39;step&#39;) axe[1][1].set_title(&quot;step&quot;) . Text(0.5, 1.0, &#39;step&#39;) . data1 = np.random.normal(0, 1, 3000) data2 = np.random.normal(-2.6, 1.8, 3000) data3 = np.random.normal(2.4, 1.5, 3000) fig, axe = plt.subplots(dpi=800) axe.hist(data1, bins=40, density=True, histtype=&#39;stepfilled&#39;, alpha=0.3, label=&quot;mu=0,std=1&quot;) axe.hist(data2, bins=40, density=True, histtype=&#39;stepfilled&#39;, alpha=0.3, label=&quot;mu=-2.6,std=1.8&quot;) axe.hist(data3, bins=40, density=True, histtype=&#39;stepfilled&#39;, alpha=0.3, label=&quot;mu=2.4,std=1.5&quot;) axe.legend() . &lt;matplotlib.legend.Legend at 0x7f404dad5588&gt; . Box Plot . A boxplot is a standardized way of displaying a dataset based on a five-number summary: the minimum, the maximum, the sample median, and the first and third quartiles. In statistics, the boxplot is a method for graphically depicting groups of numerical data through their quartiles. . Properties of Box plot . x : Array or a sequence of vectors. The input data. | vert : Set as True or False. The default value is True, which displays the boxes vertically. | labels : Sets the labels for each dataset. | notch : Set as True or False. the default value is False. If True, the parameter will produce a notched box plot. | widths : Sets the width of the box. | patch_artist : Set as True or False. the default value is False. If False, the parameter will produce boxes with the Line2D artist. Otherwise, the boxes will be drawn with Patch artists. | . labels = [&quot;Sun&quot;, &quot;Moon&quot;, &quot;Jupiter&quot;, &quot;Venus&quot;] values = [] values.append(np.random.normal(100, 10, 200)) values.append(np.random.normal(90, 20, 200)) values.append(np.random.normal(120, 25, 200)) values.append(np.random.normal(130, 30, 200)) fig, axe = plt.subplots(dpi=100) axe.boxplot(values, labels=labels) plt.show() . Customizing Box plot . boxprops: We can specify the style of the box. | whiskerprops: We can specify the style of the whisker, which is the line that connects the quartiles to the minimum/maximum. | medianprops: We can specify the style of the median (the line that indicates the median). | . labels = [&quot;Sun&quot;, &quot;Moon&quot;, &quot;Jupiter&quot;, &quot;Venus&quot;] values = [] values.append(np.random.normal(100, 10, 200)) values.append(np.random.normal(90, 20, 200)) values.append(np.random.normal(120, 25, 200)) values.append(np.random.normal(130, 30, 200)) fig, axe = plt.subplots(dpi=100) axe.boxplot(values, labels=labels,patch_artist=True, boxprops=dict(facecolor=&#39;teal&#39;, color=&#39;r&#39;)) plt.show() . Heat Maps . heatmap is a useful chart that we can use to show the relationship between two variables. Heatmap displays a general view of numerical data; it does not extract specific data points. It is a graphical representation of data where the individual values contained in a matrix are represented as colors. They can also be used to visualize missing values in data. . In order to create a heatmap, we can pass a 2-D array to imshow(). As we can see in the code below, passing the values to imshow is the core operation of the plot. . xlabels = [&quot;dog&quot;, &quot;cat&quot;, &quot;bird&quot;, &quot;fish&quot;, &quot;horse&quot;] ylabels = [&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;pink&quot;, &quot;green&quot;] values = np.array([[0.8, 1.2, 0.3, 0.9, 2.2], [2.5, 0.1, 0.6, 1.6, 0.7], [1.1, 1.3, 2.8, 0.5, 1.7], [0.2, 1.2, 1.7, 2.2, 0.5], [1.4, 0.7, 0.3, 1.8, 1.0]]) fig, axe = plt.subplots(dpi=100) axe.set_xticks(np.arange(len(xlabels))) axe.set_yticks(np.arange(len(ylabels))) axe.set_xticklabels(xlabels) axe.set_yticklabels(ylabels) im = axe.imshow(values) plt.show() . xlabels = [&quot;dog&quot;, &quot;cat&quot;, &quot;bird&quot;, &quot;fish&quot;, &quot;horse&quot;] ylabels = [&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;pink&quot;, &quot;green&quot;] values = np.array([[0.8, 1.2, 0.3, 0.9, 2.2], [2.5, 0.1, 0.6, 1.6, 0.7], [1.1, 1.3, 2.8, 0.5, 1.7], [0.2, 1.2, 1.7, 2.2, 0.5], [1.4, 0.7, 0.3, 1.8, 1.0]]) fig, axe = plt.subplots(dpi=300) axe.set_xticks(np.arange(len(xlabels))) axe.set_yticks(np.arange(len(ylabels))) axe.set_xticklabels(xlabels) axe.set_yticklabels(ylabels) im = axe.imshow(values) for i in range(len(xlabels)): for j in range(len(ylabels)): text = axe.text(i, j, values[i, j], horizontalalignment=&quot;center&quot;, verticalalignment=&quot;center&quot;, color=&quot;w&quot;) . Add a color bar to the heatmap . Although we have added some text to the chart, we still want to include a special legend to show the relationship between a color and its value. Colorbar is what we need. As we can see in the example code below, all we have to do is call axes.figure.colorbar() . xlabels = [&quot;dog&quot;, &quot;cat&quot;, &quot;bird&quot;, &quot;fish&quot;, &quot;horse&quot;] ylabels = [&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;pink&quot;, &quot;green&quot;] values = np.array([[0.8, 1.2, 0.3, 0.9, 2.2], [2.5, 0.1, 0.6, 1.6, 0.7], [1.1, 1.3, 2.8, 0.5, 1.7], [0.2, 1.2, 1.7, 2.2, 0.5], [1.4, 0.7, 0.3, 1.8, 1.0]]) fig, axe = plt.subplots(dpi=100) axe.set_xticks(np.arange(len(xlabels))) axe.set_yticks(np.arange(len(ylabels))) axe.set_xticklabels(xlabels) axe.set_yticklabels(ylabels) im = axe.imshow(values) for i in range(len(xlabels)): for j in range(len(ylabels)): text = axe.text(i, j, values[i, j], horizontalalignment=&quot;center&quot;, verticalalignment=&quot;center&quot;, color=&quot;w&quot;) axe.figure.colorbar(im, ax=axe) . &lt;matplotlib.colorbar.Colorbar at 0x7f404d802320&gt; . Drawing Spider chart . The Spider chart is useful in showing relative values for a single data point, or for comparing two or more items as they relate to various categories. Unlike many of the other plot types we’ve learned about, Matplotlib doesn’t provide a radar function. In order to draw a radar chart, we need to write the code ourselves. . Steps to Radar Chart . Step 1: Calculate the angle for the value of each category. We begin by assigning each category value an angle. The angles are accumulated according to the order in which they occur. | Step 2: Append the first item to the last, making the circular graph close. | Step 3: Set the coordinate system as polar for your figure. | Step 4: Use the plot() function to plot. We use plot() here in the same way that we used it in the lesson How to Draw a Line Plot. | Step 5: Fill the area. This step is optional. | import math labels = [&quot;Sun&quot;, &quot;Moon&quot;, &quot;Jupiter&quot;, &quot;Venus&quot;, &quot;Mars&quot;, &quot;Mecury&quot;] values=[10,8,4,5,2,7] angles = [n/float(len(labels)) * 2 *math.pi for n in range(len(labels))] # Append first value to last to create circular connection values.append(values[0]) angles.append(angles[0]) fig,ax = plt.subplots(subplot_kw=dict(polar=True),dpi=200) ax.set_xticks(angles[:-1]) ax.set_xticklabels(labels) ax.plot(angles,values) ax.fill(angles,values,&#39;skyblue&#39;,alpha=0.4) plt.show() . labels = [&quot;Sun&quot;, &quot;Moon&quot;, &quot;Jupiter&quot;, &quot;Venus&quot;, &quot;Mars&quot;, &quot;Mecury&quot;] values = [10, 8, 4, 5, 2, 7] values2 = [3, 7, 2, 8, 5, 9] values += values[:1] values2 += values2[:1] angles = [n / float(len(labels)) * 2 * math.pi for n in range(len(labels))] angles += angles[:1] fig, axe = plt.subplots(subplot_kw=dict(polar=True), dpi=200) axe.set_xticks(angles[:-1]) axe.set_xticklabels(labels, color=&#39;r&#39;) axe.plot(angles, values) axe.fill(angles, values, &#39;skyblue&#39;, alpha=0.4) axe.plot(angles, values2) axe.fill(angles, values2, &#39;teal&#39;, alpha=0.4) plt.show() . Drawing color bars . The simplest way to draw a color bar is by calling colorbar(). . Notice:: In our other lessons, most of the functions are called from the axes object. In this lesson, the colorbar() is called from the figure object. Let’s see some of the parameters required by colorbar. . `mappable`: The matplotlib.cm.ScalarMappable described by the colorbar. `cax`: The axes object onto which the color bar will be drawn. . What is matplotlib.cm.ScalarMappable? . In short, ScalarMappable is a class to map scaler data to RGBA. This class requires two parameters: . `norm`: A normalize class, which typically maps a range number to the interval [0, 1]. `cmap`: The colormap is used to map normalized data values to RGBA colors. Matplotlib has already defined many color maps, which we can use by calling plt. `get_cmap() `with a name as the parameter. . import matplotlib.pyplot as plt import matplotlib.colors fig, axe = plt.subplots(dpi=800, figsize=(12, 2)) fig.subplots_adjust(bottom=0.5) cmap = plt.get_cmap(&quot;viridis&quot;, 5) norm = matplotlib.colors.Normalize(vmin=0, vmax=1) cmapper = matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap) cmapper.set_array([]) fig.colorbar(cmapper, cax=axe, orientation=&#39;horizontal&#39;, label=&quot;viridis colormap&quot;) . &lt;matplotlib.colorbar.Colorbar at 0x7f404d6ff898&gt; . import matplotlib.pyplot as plt import matplotlib.colors fig, axe = plt.subplots(dpi=800, figsize=(10, 2)) fig.subplots_adjust(bottom=0.5) cmap = plt.get_cmap(&quot;viridis&quot;) norm = matplotlib.colors.Normalize(vmin=0, vmax=1) cmapper = matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap) cmapper.set_array([]) fig.colorbar(cmapper, cax=axe, orientation=&#39;horizontal&#39;, label=&quot;viridis colormap&quot;) . &lt;matplotlib.colorbar.Colorbar at 0x7f4050872e80&gt; . 3D Plots . Matplotlib provides many functions for drawing 3D plots. However, we will only focus on one of them in this course: the surface plot. A three-dimensional graph is essentially a plot of points in three dimensions, with data points that are triples (x,y,z)(x,y,z)(x,y,z). A surface plot is like a wireframe plot, but each face of the wireframe is a filled polygon. We can also add a colormap to the surface plot, which can aid in the perception of the topology of the surface . The basic function to plot a surface is plot_surface(). Below are some of its important parameters: . X, Y, Z: 2D arrays, the data values. | rcount, ccount: The maximum number of samples used in each direction. If the input data is large, it will downsample the data. | cmap: Sets the colormap of the surface patches. | color: Sets the color of the surface patches. | norm: Sets the normalization for the colormap. | . Notice: . In order to draw a 3D plot, we must import Axes3D from mpl_toolkits.mplot3d, like at line 1 in the example code below. This import registers the 3D projection, but is otherwise unused. | We need to set the projection=&#39;3d&#39; for our axes like at line 13. Otherwise, the code will fail. | The meshgird helps us generate all (x,y)(x,y)(x,y) pairs based on our x, y space. In short, the function creates a cartesian set for the cos(sqrt(x2+y2))cos(sqrt(x^2 + y^2))cos(sqrt(x​2​​+y​2​​)) at line 9 and 10. | . from mpl_toolkits.mplot3d import Axes3D # required X = np.linspace(-5,5,200) Y = np.linspace(-5,5,200) X,Y = np.meshgrid(X,Y) Z = np.cos(np.sqrt(X**2+Y**2)) fig,ax = plt.subplots(dpi=200) ax = fig.gca(projection=&#39;3d&#39;) ax.plot_surface(X,Y,Z) plt.show() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection. if __name__ == &#39;__main__&#39;: . Adding color bar to surface . from mpl_toolkits.mplot3d import Axes3D # required X = np.linspace(-5,5,200) Y = np.linspace(-5,5,200) X,Y = np.meshgrid(X,Y) Z = np.cos(np.sqrt(X**2+Y**2)) fig,ax = plt.subplots(dpi=200) ax = fig.gca(projection=&#39;3d&#39;) surface = ax.plot_surface(X,Y,Z,cmap=plt.get_cmap(&#39;plasma&#39;)) plt.colorbar(surface) plt.show() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection. if __name__ == &#39;__main__&#39;: . Fill between curves . In order to draw a confidence region, the basic function to call is fill_between(). Below are some of the important parameters required by the function: . x : The x coordinates of the nodes defining the curves. | y1 : The y coordinates of the nodes defining the first curve. | y2 : The y coordinates of the nodes defining the second curve. | where : Array of bool. Defines where to exclude some horizontal regions from being filled. For example, fill between x[i] and x[i+1] if where[i] and where[i+1] is True. | . x = np.linspace(-5,5,200) y1 = np.sin(x) fig,ax = plt.subplots(dpi=100) ax.plot(x,y1) ax.fill_between(x,y1,color=&#39;green&#39;,alpha=0.4) . &lt;matplotlib.collections.PolyCollection at 0x7f404d20cef0&gt; . x = np.linspace(-5,5,200) y1 = np.sin(x) fig,ax = plt.subplots(dpi=100) ax.plot(x,y1,color=&#39;black&#39;) ax.fill_between(x,y1,where=(y1&gt;0),color=&#39;green&#39;,alpha=0.4) ax.fill_between(x,y1,where=(y1&lt;0),color=&#39;red&#39;,alpha=0.4) . &lt;matplotlib.collections.PolyCollection at 0x7f404d3bb1d0&gt; . How to draw a confidence band . We will be able to draw a confidence band if we follow the steps below: . Step 1: Prepare the data that we need. In the example code below, our data is x and y at line 4 and line 5. . Step 2: Fit our data. NumPy provides the useful function polyfit. On our linear data set, we can fit a slope and an intercept, which we’ve done with a and b at line 7. . Notice: polyfit is a least squares polynomial fit, which it a polynomial p(x) = p[0] x^{0} + p[1]x^{1} ... +p[n] to points (x, y). Polyfit returns a vector of coefficients, pp, that minimizes the squared error. More details can be found on the official Matplotlib site. . Step 3: Get an estimation curve based on a and b, which we’ve done with y_est at line 8. . Step 4: Get the error, which we’ve done with y_err at line 9. . Step 5: Draw the plot by using fill_between(). The band is between $ y _est - y _erry_est−y_err and y _est + y _erry_est+y_err$ which we’ve set at line 14. . x = np.linspace(0, 10, 11) y = [3.9, 4.4, 10.8, 10.3, 11.2, 13.1, 14.1, 9.9, 13.9, 15.1, 12.5] # step 2 a,b = np.polyfit(x,y,1) # step 3 y_est = a * x + b # set 4 y_err = x.std() * np.sqrt((x - x.mean())**2 / np.sum((x - x.mean())**2) + (1/len(x))) # step 4 fig,ax = plt.subplots(dpi=100) ax.plot(x,y_est,&#39;-&#39;) ax.fill_between(x, y_est - y_err, y_est + y_err, alpha=0.2) ax.scatter(x,y) plt.show() . Stacked Plot . The idea of stack plots is to show “parts to a whole” over time; basically, it&#39;s like a pie-chart, only over time. Stack plots are mainly used to see various trends in variables over a specific period of time. . Properties . stackplot() is the basic function that Matplotlib provides to create a stack plot. Below are some of the important parameters required by the function: . x: 1d array of dimension $N$. . y: 2d array, dimension$(M times M)$,or a sequence of arrays. Each dimension is $1 times N$. . stackplot(x, y) stackplot(x, y1, y2, y3) . baseline: The method used to calculate the baseline. . zero a constant zero baseline, which is the default setting. | sym sets the plot symmetrically around zero. | wiggle minimizes the sum of the squared slopes. | . colors: Provides a list of colors for each data set. . x = [1, 2, 3, 4, 5] y = [1, 2, 4, 8, 16] y1 = y+np.random.randint(1,5,5) y2 = y+ np.random.randint(1,5,5) y3 = y+np.random.randint(1,5,5) y4 = y+np.random.randint(1,5,5) y5 = y+np.random.randint(1,5,5) y6 = y+np.random.randint(1,5,5) labels = [&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;] fig,ax = plt.subplots(dpi=200) ax.stackplot(x,y,y1,y2,y3,y4,y5,y6,labels=[&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;]) ax.set_xticks(x) ax.set_xticklabels(labels) ax.legend(loc=&#39;upper left&#39;) . &lt;matplotlib.legend.Legend at 0x7f404d0e19e8&gt; . Different baseline . In the example above, the baseline we used is zero, which is the default option. In the example below, we will learn about the other two options, sym and wiggle. In order the change the position of the baseline, we pass the corresponding string to the parameter baseline. . np.random.seed(42) x = [1, 2, 3, 4, 5] y = [1, 2, 4, 8, 16] y1 = y+np.random.randint(1,5,5) y2 = y+ np.random.randint(1,5,5) y3 = y+np.random.randint(1,5,5) y4 = y+np.random.randint(1,25,5) y5 = y+np.random.randint(1,15,5) y6 = y+np.random.randint(1,20,5) labels = [&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;] fig, axe = plt.subplots(nrows=2, dpi=200) plt.tight_layout() axe[0].stackplot(x, y, y1, y2, y3, y4, y5, y6, baseline=&quot;sym&quot;) axe[0].set_xticks(x) axe[0].set_xticklabels(labels) axe[0].set_title(&quot;symmetric&quot;) axe[1].stackplot(x, y, y1, y2, y3, y4, y5, y6, baseline=&quot;wiggle&quot;) axe[1].set_xticks(x) axe[1].set_xticklabels(labels) axe[1].set_title(&quot;wiggle&quot;) . Text(0.5, 1.0, &#39;wiggle&#39;) .",
            "url": "https://isbhargav.github.io/portfolio/jupyter/matplotlib/2020/11/15/Matplotlib2.html",
            "relUrl": "/jupyter/matplotlib/2020/11/15/Matplotlib2.html",
            "date": " • Nov 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Matplotlib Guide - p1",
            "content": "What is Matplotlib? . Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension, NumPy. Matplotlib is a powerful and widely used library that provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+. . This Notebook covers following topics: . Plot charts, individually and in multiples. | Set the Tick, Text, Legend, and Annotate elements of a plot. | Customize the Grid and Spine displays of a plot. | Set the colors for different elements of a plot | Customize the style and appearance of different plot components. | Choose different chart types based on data type and requirement. | Show the distribution of data. | Show the relationship between different variables. | Plot data in 2D and 3D. | . . Anotomy of matplotlib . Figure . The whole Figure, which is marked by the red box, is like a canvas. Everything that we want to draw will be on this canvas. The figure can contain one or more axes plots. Regardless of how many we draw, the figure controls all of the axes. . Axes . The axes are marked by the blue box. They are where the data will appear. While one figure can contain more than one axes, the axes can only belong to one figure. . Axis . The x-axis and the y-axis, both circled in green, are number-line-like objects. They set the graph limits and generate the ticks (the marks on the axis) and the tick labels (the strings labeling the ticks). . Artist . Artist is the collection of objects that know how to use a renderer to paint the canvas or figure. Artist encompasses many objects, such as the title, legend, axis, spine, grid, and tick . . import numpy as np import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) %matplotlib inline . How to Draw a figure? . To draw a figure in matplotlib you need to follow these basic steps in folowing order . Create dataset | Create canvas | Add data to axes | Show figure | points = np.linspace(-5,5,256) y1 = np.tanh(points) + 0.5 y2 = np.sin(points) - 0.2 # 2. Create canvas/figure fig,ax = plt.subplots(1,1,figsize=(15,9)) # 3. Add data to axes (x,y) ax.hlines(0,-6,6) ax.plot(points,y1,label=&quot;tanh&quot;) ax.plot(points,y2,color=&quot;green&quot;,marker=&#39;.&#39;,linestyle=&#39;dashed&#39;,label=&quot;sin&quot;) # 4. Show the figure plt.show() # To save the image use following lines # fig.savefig(&#39;output/to.png&#39;) # plt.close(fig) . What are ticks? . Ticks are the marks on the x and y axis of the plot. The tick can be a number, a string, or any other notation. When plotting a graph, the axes will set a default tick based on the number ranges of the different axes. . The following are some properties that we can manipulate by setting parameters: . which: Sets the x/y major/minor ticks. The default value is major. | color: Sets the tick color. | labelrotation: Rotates the tick label. | width: Sets the tick width in points. | length: Sets the tick length in points. | direction: Sets the direction of the ticks, either towards the axis or away from it. The default value is out. | . points = np.linspace(-5, 5, 256) y1 = np.tanh(points) + 0.5 y2 = np.sin(points) - 0.2 fig,ax = plt.subplots(dpi=600,figsize=(15,9)) ax.plot(points,y1) ax.plot(points,y2) # Set x Tick ax.set_xticks(np.linspace(-5,5,9)) plt.show() . Changing tick parmas with tick_params() . points = np.linspace(-5, 5, 256) y1 = np.tanh(points) + 0.5 y2 = np.sin(points) - 0.2 fig,ax = plt.subplots(nrows=1,ncols=2,dpi=400,figsize=(10,6)) ax[0].plot(points,y1) ax[0].plot(points,y2) ax[0].set_title(&quot;Original&quot;) ax[1].plot(points,y1) ax[1].plot(points,y2) ax[1].set_title(&quot;Pretty&quot;) ax[1].set_xticks(np.linspace(-5,5,9)) ax[1].tick_params(color=&#39;b&#39;,length=5, width=2,direction=&#39;in&#39;) plt.show() . What is the spine? . Spines are lines that connect the axis tick marks to one another. Spines also note the boundaries of the data area. They can be placed in any position we want. By default, they are the boundary of the entire axes. So, for an axes, we have four spines: . top | bottom | left | right | . set_position() accepts a tuple of (position type, amount) as parameter. The position type has the following types. . axes: places the spine at the specified axes coordinate (from 0.0-1.0) | data: places the spine at the specified data coordinate. | outward: places the spine out from the data area by the specified number of points. | . points = np.linspace(-5, 5, 256) y1 = np.tanh(points) + 0.5 y2 = np.sin(points) - 0.2 fig,ax = plt.subplots(dpi=200) ax.plot(points,y1) ax.plot(points,y2) ax.set_xticks(np.linspace(-5,5,9)) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;left&#39;].set_position((&#39;data&#39;,0)) ax.spines[&#39;left&#39;].set_color(&#39;black&#39;) ax.spines[&#39;bottom&#39;].set_position((&#39;axes&#39;,0.5)) # we could use (data,0) ax.spines[&#39;bottom&#39;].set_color(&#39;black&#39;) plt.show() . points = np.linspace(-5, 5, 256) y1 = np.tanh(points) + 0.5 y2 = np.sin(points) - 0.2 fig, axe = plt.subplots(nrows=1, ncols=2, figsize=(14, 3.5), dpi=400) axe[0].plot(points, y1) axe[0].plot(points, y2) axe[1].plot(points, y1) axe[1].plot(points, y2) axe[1].set_xticks(np.linspace(-5, 5, 7)) axe[1].tick_params(width=0.5, colors=&#39;b&#39;) axe[1].spines[&#39;right&#39;].set_color(&#39;none&#39;) axe[1].spines[&#39;top&#39;].set_color(&#39;none&#39;) axe[1].spines[&#39;left&#39;].set_position((&#39;data&#39;, 0)) axe[1].spines[&#39;bottom&#39;].set_position((&#39;axes&#39;, 0.5)) ax.spines[&#39;left&#39;].set_color(&#39;black&#39;) ax.spines[&#39;bottom&#39;].set_color(&#39;black&#39;) . What is legend? . A legend provides a description for an element in the figure. A legend helps the user understand a figure when the figure contains many elements. . fig,(ax1,ax2) = plt.subplots(1,2,dpi=200,figsize=(10,6)) ax1.plot(points,y1) ax1.plot(points,y2) ax2.plot(points,y1,label=&quot;tanh&quot;) ax2.plot(points,y2,label=&quot;sin&quot;) ax2.legend() plt.show() . Customizing legend properties . As with other elements in a figure, legend supports many customizable properties, including location, display style, box style, and shadow. The following are some of the most used properties: . loc: The location of the legend. The most important and commonly used attribute. The strings &#39;upper left&#39;, &#39;upper right&#39;, &#39;lower left&#39;, &#39;lower right&#39; place the legend at the corresponding corner of the axes/figure. The strings &#39;upper center&#39;, &#39;lower center&#39;, &#39;center left&#39;, &#39;center right&#39; place the legend at the center of the corresponding edge of the axes/figure. | fontsize: The font size of the legend. | ncol: The number of columns that the legend will have. The default value is 1. | frameon: Controls whether the legend will be drawn with frame. | shadow: Controls whether a shadow will be drawn behind the legend. | title: Sets a title for the legend. | facecolor: Sets the legend’s background color. | edgecolor: Sets the legend’s frame color. | . ax.legend(loc=&#39;upper center&#39;, title=&quot;This is a legend&quot;, shadow=True, ncol=2, facecolor=&#39;r&#39;) . fig, axe = plt.subplots(nrows=1, ncols=2, dpi=800, figsize=(12, 4)) axe[0].plot(points, y1) axe[0].plot(points, y2) axe[0].set_xticks(np.linspace(-5, 5, 9)) axe[0].tick_params(width=2, colors=&#39;b&#39;) axe[0].spines[&#39;right&#39;].set_color(&#39;none&#39;) axe[0].spines[&#39;top&#39;].set_color(&#39;none&#39;) axe[0].spines[&#39;left&#39;].set_position((&#39;data&#39;, 0)) axe[0].spines[&#39;bottom&#39;].set_position((&#39;axes&#39;, 0.5)) axe[1].plot(points, y1, label=&quot;tanh&quot;) axe[1].plot(points, y2, label=&quot;sin&quot;) axe[1].set_xticks(np.linspace(-5, 5, 9)) axe[1].tick_params(width=2, colors=&#39;b&#39;) axe[1].spines[&#39;right&#39;].set_color(&#39;none&#39;) axe[1].spines[&#39;top&#39;].set_color(&#39;none&#39;) axe[1].spines[&#39;left&#39;].set_position((&#39;data&#39;, 0)) axe[1].spines[&#39;bottom&#39;].set_position((&#39;axes&#39;, 0.5)) ax.spines[&#39;left&#39;].set_color(&#39;black&#39;) ax.spines[&#39;bottom&#39;].set_color(&#39;black&#39;) axe[1].legend(loc=&quot;upper left&quot;,frameon=False ) plt.show() . What is annotate? . Annotate is a piece of text that identifies a specific data point on a graph, designed to help users understand charts. For example, annotate can be used to give a detailed explanation of the inflection point of a chart. . Adding a basic annotation . xy : The point (x, y) to be annotated, which is a tuple. | xytext : The position (x,y) where the text will appear. None defaults to xy. | xycoords : The coordinate system that xy is given in.(xycoords=‘data’ ) | textcoords : The coordinate system that xytext is given in. | arrowprops : The properties used to draw an arrow between xy and xytext. | . fig,ax = plt.subplots(dpi=200) ax.plot(points,y1) ax.plot(points,y2) ax.annotate(&quot;1.464=tanh(2)+0.5&quot;,xy=(2,1.468),xycoords=&quot;data&quot;,xytext=(0.4,-40),textcoords=&#39;offset points&#39;,arrowprops=dict(arrowstyle=&quot;-&gt;&quot;,color=&quot;black&quot;, connectionstyle=&quot;arc3,rad=.5&quot;)) . Text(0.4, -40, &#39;1.464=tanh(2)+0.5&#39;) . Changing the style of the annotation by using bbox . we’ve set the border to have rounded corners by using boxstyle=&quot;round&quot;. . The fc stands for fore color, which we’ve set as invisible. . The ec stands for edge color, which we’ve set to gray. . fig, axe = plt.subplots(dpi=800, figsize=(9, 5)) axe.plot(points, y1) axe.plot(points, y2) axe.legend([&quot;tanh&quot;, &quot;sin&quot;]) axe.annotate(&quot;style1&quot;, xy=(2, 1.464), xycoords=&quot;data&quot;, xytext=(0.4, -40), textcoords=&#39;offset points&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.5&quot;,color=&quot;black&quot;), bbox=dict(boxstyle=&quot;round&quot;, fc=&quot;none&quot;, ec=&quot;gray&quot;)) axe.annotate(&quot;style2&quot;, xy=(2, 1.464), xycoords=&quot;data&quot;, xytext=(-200, -40), textcoords=&#39;offset points&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.5&quot;,color=&quot;black&quot;), bbox=dict(boxstyle=&quot;round&quot;, alpha=0.1),) axe.annotate(&quot;style3&quot;, xy=(2, 1.464), xycoords=&quot;data&quot;, xytext=(-160, -80), textcoords=&#39;offset points&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.5&quot;,color=&quot;black&quot;), bbox=dict(boxstyle=&quot;round4,pad=.5&quot;, fc=&quot;0.3&quot;)) . Text(-160, -80, &#39;style3&#39;) . What is text? . Text is basically a simpler version of annotate. Unlike annotate, we don’t need to anchor our text to one specific data location. . text(0.5, 0.5, &#39;function&#39;, horizontalalignment=&#39;center&#39;,verticalalignment=&#39;center&#39;, transform=ax.transAxes) . If we want to put a frame around the text, we can use the option bbox. The example code below adds a box with a red background to our text. . text(10, 20, &quot;function&quot;, bbox=dict(facecolor=&#39;red&#39;, alpha=0.5)) . fig, axe = plt.subplots(figsize=(7, 3.5), dpi=300) axe.plot(points, y1) axe.plot(points, y2) axe.legend([&quot;tanh&quot;, &quot;sin&quot;]) axe.text(-2.5, 0.5, &quot;two function nOne nTwo&quot;, bbox=dict(facecolor=&#39;red&#39;, alpha=0.5)) plt.show() . fig, axe = plt.subplots(figsize=(7, 3.5), dpi=800) axe.plot(points, y1) axe.plot(points, y2) axe.legend([&quot;tanh&quot;, &quot;sin&quot;]) eq = r&quot;$ int_a^b{ sin(x)} dx$&quot; font = {&#39;family&#39;: &#39;serif&#39;, &#39;color&#39;: &#39;darkred&#39;, &#39;weight&#39;: &#39;normal&#39;, &#39;size&#39;: 16, } axe.text(-3, 0.18, eq, font) . Text(-3, 0.18, &#39;$ int_a^b{ sin(x)} dx$&#39;) . What is grid? . The grid consists of lines that connect to each of the ticks that we have set. These lines help the user locate the curve in the diagram more quickly, making the image more readable. . fig, axe = plt.subplots(figsize=(7, 3.5), dpi=300) axe.plot(points, y1) axe.plot(points, y2) axe.legend([&quot;tanh&quot;, &quot;sin&quot;]) axe.annotate(&quot;1.464=tanh(2)+0.5&quot;, xy=(2, 1.464), xycoords=&quot;data&quot;, xytext=(0.4, -40), textcoords=&#39;offset points&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.5&quot;)) axe.minorticks_on() axe.grid(which=&#39;major&#39;, linestyle=&#39;-&#39;, linewidth=&#39;0.5&#39;, color=&#39;blue&#39;) axe.grid(which=&#39;minor&#39;, linestyle=&#39;:&#39;, linewidth=&#39;0.5&#39;, color=&#39;red&#39;) . Adding a title for the axes . fig, axe = plt.subplots(figsize=(7, 3.5), dpi=300) axe.plot(points, y1) axe.plot(points, y2) axe.legend([&quot;tanh&quot;, &quot;sin&quot;]) axe.set_title(&quot;two functions&quot;) axe.set_title(&quot;two functions left&quot;, loc=&quot;left&quot;) axe.set_title(&quot;two functions right&quot;, loc=&quot;right&quot;) . Text(1.0, 1.0, &#39;two functions right&#39;) . fig, axe = plt.subplots(1,2, figsize=(14, 3.5), dpi=300) axe[0].plot(points, y1) axe[0].set_title(&quot;tanh functions&quot;) axe[1].plot(points, y2) axe[1].set_title(&quot;sin functions&quot;) fig.suptitle(&quot;tanh &amp; sin function&quot;) . Text(0.5, 0.98, &#39;tanh &amp; sin function&#39;) . Placing Multiple Plots onto One Figure Using Subplots . In order to prevent clipping, we can call plt.tight_layout(). The constrained layout helps us adjust our elements to fit them comfortably onto the figure. . fig, axe = plt.subplots(nrows=2, ncols=2, dpi=800) plt.tight_layout() axe[1][0].set_title(&quot;the third subplot&quot;) axe[1][1].set_title(&quot;the forth subplot&quot;) . Text(0.5, 1.0, &#39;the forth subplot&#39;) . Colors . Different ways to specify colors . x = np.linspace(-4, 4, 200) fig, axe = plt.subplots(dpi=300) # RGB axe.plot(x, x + 2, color=(0.1, 0.6, 1.0), label=&quot;RGB&quot;) # HEX RGB axe.plot(x, x + 3, color=&quot;#6F4F3F&quot;, label=&quot;Hex&quot;) # single color letter axe.plot(x, x + 4, color=&#39;r&#39;, label=&quot;single letter&quot;) # color name axe.plot(x, x + 5, color=&#39;green&#39;, label=&quot;color name&quot;) # Tableau color axe.plot(x, x + 6, color=&#39;tab:olive&#39;, label=&quot;Tableau&quot;) # gray level axe.plot(x, x + 7, color=&#39;0.7&#39;, label=&quot;gray level&quot;) . [&lt;matplotlib.lines.Line2D at 0x7fde586af978&gt;] .",
            "url": "https://isbhargav.github.io/portfolio/jupyter/matplotlib/2020/11/14/matplotlib_p1.html",
            "relUrl": "/jupyter/matplotlib/2020/11/14/matplotlib_p1.html",
            "date": " • Nov 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Predicting Crime Location",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import holidays from datetime import datetime,date from sklearn.neighbors import KNeighborsClassifier import warnings from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder from pyproj import Proj import folium warnings.filterwarnings(&quot;ignore&quot;) . df = pd.read_csv(&#39;crimedata_csv_all_years.csv&#39;) df.head(10) . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y . 0 | Break and Enter Commercial | 2012 | 12 | 14 | 8 | 52 | NaN | Oakridge | 491285.000000 | 5.453433e+06 | . 1 | Break and Enter Commercial | 2019 | 3 | 7 | 2 | 6 | 10XX SITKA SQ | Fairview | 490612.964805 | 5.457110e+06 | . 2 | Break and Enter Commercial | 2019 | 8 | 27 | 4 | 12 | 10XX ALBERNI ST | West End | 491007.779775 | 5.459174e+06 | . 3 | Break and Enter Commercial | 2014 | 8 | 8 | 5 | 13 | 10XX ALBERNI ST | West End | 491015.943352 | 5.459166e+06 | . 4 | Break and Enter Commercial | 2005 | 11 | 14 | 3 | 9 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | . 5 | Break and Enter Commercial | 2006 | 5 | 21 | 4 | 50 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | . 6 | Break and Enter Commercial | 2009 | 7 | 1 | 0 | 35 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | . 7 | Break and Enter Commercial | 2013 | 6 | 1 | 20 | 0 | 10XX ALBERNI ST | West End | 491032.270497 | 5.459150e+06 | . 8 | Break and Enter Commercial | 2014 | 4 | 17 | 5 | 50 | 10XX ALBERNI ST | West End | 491032.270497 | 5.459150e+06 | . 9 | Break and Enter Commercial | 2014 | 9 | 1 | 14 | 20 | 10XX ALBERNI ST | West End | 491032.270497 | 5.459150e+06 | . df.isnull().sum() . TYPE 0 YEAR 0 MONTH 0 DAY 0 HOUR 0 MINUTE 0 HUNDRED_BLOCK 13 NEIGHBOURHOOD 64574 X 119 Y 119 dtype: int64 . Drop all instaces with missing X,Y and Hundred Block . df.dropna(subset=[&#39;X&#39;,&#39;Y&#39;,&#39;HUNDRED_BLOCK&#39;],inplace=True) . df.isnull().sum() . TYPE 0 YEAR 0 MONTH 0 DAY 0 HOUR 0 MINUTE 0 HUNDRED_BLOCK 0 NEIGHBOURHOOD 64455 X 0 Y 0 dtype: int64 . Converting all X,Y from UTM to lat,lng . p = Proj(proj=&#39;utm&#39;,zone=10,ellps=&#39;WGS84&#39;, preserve_units=False) . lat_lng = [] _ = df[[&#39;X&#39;,&#39;Y&#39;]].apply(lambda x: lat_lng.append(p(x.X,x.Y,inverse=True)),axis=1) . lng = [] lat = [] for i in range(len(lat_lng)): lng.append(lat_lng[i][0]) lat.append(lat_lng[i][1]) df[&#39;LAT&#39;] = lat df[&#39;LNG&#39;] = lng . df.head(20) . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y LAT LNG . 1 | Break and Enter Commercial | 2019 | 3 | 7 | 2 | 6 | 10XX SITKA SQ | Fairview | 490612.964805 | 5.457110e+06 | 49.266678 | -123.129029 | . 2 | Break and Enter Commercial | 2019 | 8 | 27 | 4 | 12 | 10XX ALBERNI ST | West End | 491007.779775 | 5.459174e+06 | 49.285255 | -123.123649 | . 3 | Break and Enter Commercial | 2014 | 8 | 8 | 5 | 13 | 10XX ALBERNI ST | West End | 491015.943352 | 5.459166e+06 | 49.285181 | -123.123536 | . 4 | Break and Enter Commercial | 2005 | 11 | 14 | 3 | 9 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | . 5 | Break and Enter Commercial | 2006 | 5 | 21 | 4 | 50 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | . 6 | Break and Enter Commercial | 2009 | 7 | 1 | 0 | 35 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | . 7 | Break and Enter Commercial | 2013 | 6 | 1 | 20 | 0 | 10XX ALBERNI ST | West End | 491032.270497 | 5.459150e+06 | 49.285034 | -123.123311 | . 8 | Break and Enter Commercial | 2014 | 4 | 17 | 5 | 50 | 10XX ALBERNI ST | West End | 491032.270497 | 5.459150e+06 | 49.285034 | -123.123311 | . 9 | Break and Enter Commercial | 2014 | 9 | 1 | 14 | 20 | 10XX ALBERNI ST | West End | 491032.270497 | 5.459150e+06 | 49.285034 | -123.123311 | . 10 | Break and Enter Commercial | 2017 | 11 | 14 | 20 | 0 | 10XX ALBERNI ST | West End | 491051.085574 | 5.459144e+06 | 49.284981 | -123.123053 | . 11 | Break and Enter Commercial | 2018 | 3 | 2 | 6 | 17 | 10XX ALBERNI ST | West End | 491058.816893 | 5.459123e+06 | 49.284794 | -123.122946 | . 12 | Break and Enter Commercial | 2005 | 2 | 8 | 18 | 0 | 10XX ALBERNI ST | West End | 491059.482407 | 5.459122e+06 | 49.284788 | -123.122937 | . 13 | Break and Enter Commercial | 2006 | 4 | 28 | 15 | 0 | 10XX ALBERNI ST | West End | 491059.482407 | 5.459122e+06 | 49.284788 | -123.122937 | . 14 | Break and Enter Commercial | 2007 | 3 | 13 | 16 | 38 | 10XX ALBERNI ST | West End | 491059.482407 | 5.459122e+06 | 49.284788 | -123.122937 | . 15 | Break and Enter Commercial | 2007 | 5 | 15 | 0 | 1 | 10XX ALBERNI ST | West End | 491059.482407 | 5.459122e+06 | 49.284788 | -123.122937 | . 16 | Break and Enter Commercial | 2010 | 2 | 8 | 11 | 4 | 10XX ALBERNI ST | West End | 491059.482407 | 5.459122e+06 | 49.284788 | -123.122937 | . 17 | Break and Enter Commercial | 2013 | 11 | 6 | 23 | 0 | 10XX ALBERNI ST | West End | 491059.482407 | 5.459122e+06 | 49.284788 | -123.122937 | . 18 | Break and Enter Commercial | 2014 | 11 | 22 | 23 | 28 | 10XX ALBERNI ST | West End | 491059.482407 | 5.459122e+06 | 49.284788 | -123.122937 | . 19 | Break and Enter Commercial | 2003 | 9 | 26 | 2 | 30 | 10XX ALBERNI ST | West End | 491067.645985 | 5.459114e+06 | 49.284715 | -123.122824 | . 20 | Break and Enter Commercial | 2004 | 1 | 24 | 0 | 1 | 10XX ALBERNI ST | West End | 491067.645985 | 5.459114e+06 | 49.284715 | -123.122824 | . Using KNN with havesine distance . train_df = df.dropna(subset=[&#39;NEIGHBOURHOOD&#39;]) test_df=df[df[&#39;NEIGHBOURHOOD&#39;].isnull()] test_df.head() . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y LAT LNG . 38536 | Break and Enter Commercial | 2007 | 9 | 23 | 18 | 5 | X NK_LOC ST | NaN | 492757.48676 | 5.458792e+06 | 49.281843 | -123.099582 | . 38537 | Break and Enter Commercial | 2007 | 11 | 1 | 14 | 38 | X NK_LOC ST | NaN | 492757.48676 | 5.458792e+06 | 49.281843 | -123.099582 | . 38538 | Break and Enter Commercial | 2005 | 5 | 18 | 18 | 30 | X NK_LOC ST &quot;SQUAMISH&quot; | NaN | 492757.48676 | 5.458792e+06 | 49.281843 | -123.099582 | . 104648 | Break and Enter Residential/Other | 2004 | 6 | 3 | 14 | 20 | X NK_LOC ST | NaN | 492757.48676 | 5.458792e+06 | 49.281843 | -123.099582 | . 104649 | Break and Enter Residential/Other | 2008 | 3 | 13 | 7 | 0 | X NK_LOC ST | NaN | 492757.48676 | 5.458792e+06 | 49.281843 | -123.099582 | . x_train = train_df[[&#39;LAT&#39;,&#39;LNG&#39;]] y_train = train_df[&#39;NEIGHBOURHOOD&#39;] . from sklearn.neighbors import KNeighborsClassifier knn_classifier = KNeighborsClassifier(n_neighbors=5,metric=&#39;haversine&#39;) knn_classifier.fit(x_train, y_train) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . pred_neig = knn_classifier.predict(test_df[[&#39;LAT&#39;,&#39;LNG&#39;]]) . test_df[&#39;NEIGHBOURHOOD&#39;] = pred_neig pred_neig[:20] . array([&#39;Central Business District&#39;, &#39;Central Business District&#39;, &#39;Central Business District&#39;, &#39;Central Business District&#39;, &#39;Central Business District&#39;, &#39;Central Business District&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;, &#39;Marpole&#39;], dtype=object) . new_df = df[0:0] new_df=pd.concat([new_df,train_df],ignore_index=True) new_df=pd.concat([new_df,test_df],ignore_index=True) new_df.isnull().sum() . TYPE 0 YEAR 0 MONTH 0 DAY 0 HOUR 0 MINUTE 0 HUNDRED_BLOCK 0 NEIGHBOURHOOD 0 X 0 Y 0 LAT 0 LNG 0 dtype: int64 . Feature engineering . Adding new date feature . new_df[&#39;DATE&#39;] = new_df.apply(lambda r: datetime(year=r.YEAR,month=r.MONTH,day=r.DAY,hour=r.HOUR,minute=r.MINUTE),axis=1) new_df.head() . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y LAT LNG DATE . 0 | Break and Enter Commercial | 2019 | 3 | 7 | 2 | 6 | 10XX SITKA SQ | Fairview | 490612.964805 | 5.457110e+06 | 49.266678 | -123.129029 | 2019-03-07 02:06:00 | . 1 | Break and Enter Commercial | 2019 | 8 | 27 | 4 | 12 | 10XX ALBERNI ST | West End | 491007.779775 | 5.459174e+06 | 49.285255 | -123.123649 | 2019-08-27 04:12:00 | . 2 | Break and Enter Commercial | 2014 | 8 | 8 | 5 | 13 | 10XX ALBERNI ST | West End | 491015.943352 | 5.459166e+06 | 49.285181 | -123.123536 | 2014-08-08 05:13:00 | . 3 | Break and Enter Commercial | 2005 | 11 | 14 | 3 | 9 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | 2005-11-14 03:09:00 | . 4 | Break and Enter Commercial | 2006 | 5 | 21 | 4 | 50 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | 2006-05-21 04:50:00 | . Adding Weekday field . new_df[&#39;WEEKDAY&#39;] = new_df[&#39;DATE&#39;].dt.dayofweek new_df.head() . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y LAT LNG DATE WEEKDAY . 0 | Break and Enter Commercial | 2019 | 3 | 7 | 2 | 6 | 10XX SITKA SQ | Fairview | 490612.964805 | 5.457110e+06 | 49.266678 | -123.129029 | 2019-03-07 02:06:00 | 3 | . 1 | Break and Enter Commercial | 2019 | 8 | 27 | 4 | 12 | 10XX ALBERNI ST | West End | 491007.779775 | 5.459174e+06 | 49.285255 | -123.123649 | 2019-08-27 04:12:00 | 1 | . 2 | Break and Enter Commercial | 2014 | 8 | 8 | 5 | 13 | 10XX ALBERNI ST | West End | 491015.943352 | 5.459166e+06 | 49.285181 | -123.123536 | 2014-08-08 05:13:00 | 4 | . 3 | Break and Enter Commercial | 2005 | 11 | 14 | 3 | 9 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | 2005-11-14 03:09:00 | 0 | . 4 | Break and Enter Commercial | 2006 | 5 | 21 | 4 | 50 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | 2006-05-21 04:50:00 | 6 | . Adding Holiday field . ca_holidays = holidays.Canada(prov=&#39;BC&#39;) new_df[&#39;HOLIDAY&#39;] = new_df.apply(lambda r: 1 if r.DATE in ca_holidays else 0,axis=1) new_df.head() . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y LAT LNG DATE WEEKDAY HOLIDAY . 0 | Break and Enter Commercial | 2019 | 3 | 7 | 2 | 6 | 10XX SITKA SQ | Fairview | 490612.964805 | 5.457110e+06 | 49.266678 | -123.129029 | 2019-03-07 02:06:00 | 3 | 0 | . 1 | Break and Enter Commercial | 2019 | 8 | 27 | 4 | 12 | 10XX ALBERNI ST | West End | 491007.779775 | 5.459174e+06 | 49.285255 | -123.123649 | 2019-08-27 04:12:00 | 1 | 0 | . 2 | Break and Enter Commercial | 2014 | 8 | 8 | 5 | 13 | 10XX ALBERNI ST | West End | 491015.943352 | 5.459166e+06 | 49.285181 | -123.123536 | 2014-08-08 05:13:00 | 4 | 0 | . 3 | Break and Enter Commercial | 2005 | 11 | 14 | 3 | 9 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | 2005-11-14 03:09:00 | 0 | 0 | . 4 | Break and Enter Commercial | 2006 | 5 | 21 | 4 | 50 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | 2006-05-21 04:50:00 | 6 | 0 | . Hashing latitude and longitude to geohash . import geohash_hilbert as ghh new_df[&#39;GEOHASH&#39;] = new_df.apply(lambda r: ghh.encode(r.LNG, r.LAT, precision=8,bits_per_char=4),axis=1) new_df.head() . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y LAT LNG DATE WEEKDAY HOLIDAY CRIME_TYPE GEOHASH . 0 | Break and Enter Commercial | 2019 | 3 | 7 | 2 | 6 | 10XX SITKA SQ | Fairview | 490612.964805 | 5.457110e+06 | 49.266678 | -123.129029 | 2019-03-07 02:06:00 | 3 | 0 | B&amp;E | 5ed43e02 | . 1 | Break and Enter Commercial | 2019 | 8 | 27 | 4 | 12 | 10XX ALBERNI ST | West End | 491007.779775 | 5.459174e+06 | 49.285255 | -123.123649 | 2019-08-27 04:12:00 | 1 | 0 | B&amp;E | 5ed43e44 | . 2 | Break and Enter Commercial | 2014 | 8 | 8 | 5 | 13 | 10XX ALBERNI ST | West End | 491015.943352 | 5.459166e+06 | 49.285181 | -123.123536 | 2014-08-08 05:13:00 | 4 | 0 | B&amp;E | 5ed43e44 | . 3 | Break and Enter Commercial | 2005 | 11 | 14 | 3 | 9 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | 2005-11-14 03:09:00 | 0 | 0 | B&amp;E | 5ed43e44 | . 4 | Break and Enter Commercial | 2006 | 5 | 21 | 4 | 50 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | 2006-05-21 04:50:00 | 6 | 0 | B&amp;E | 5ed43e44 | . (np.unique(new_df.NEIGHBOURHOOD)) . array([&#39;Arbutus Ridge&#39;, &#39;Central Business District&#39;, &#39;Dunbar-Southlands&#39;, &#39;Fairview&#39;, &#39;Grandview-Woodland&#39;, &#39;Hastings-Sunrise&#39;, &#39;Kensington-Cedar Cottage&#39;, &#39;Kerrisdale&#39;, &#39;Killarney&#39;, &#39;Kitsilano&#39;, &#39;Marpole&#39;, &#39;Mount Pleasant&#39;, &#39;Musqueam&#39;, &#39;Oakridge&#39;, &#39;Renfrew-Collingwood&#39;, &#39;Riley Park&#39;, &#39;Shaughnessy&#39;, &#39;South Cambie&#39;, &#39;Stanley Park&#39;, &#39;Strathcona&#39;, &#39;Sunset&#39;, &#39;Victoria-Fraserview&#39;, &#39;West End&#39;, &#39;West Point Grey&#39;], dtype=object) . new_df_copy = new_df.copy() . for col in list(np.unique(new_df.NEIGHBOURHOOD)): print(col,len(new_df[new_df[&#39;NEIGHBOURHOOD&#39;]==col]),len(new_df.GEOHASH[new_df[&#39;NEIGHBOURHOOD&#39;]==col].value_counts())) . Arbutus Ridge 6790 47 Central Business District 138630 49 Dunbar-Southlands 8752 65 Fairview 36503 44 Grandview-Woodland 31413 54 Hastings-Sunrise 21126 82 Kensington-Cedar Cottage 28232 70 Kerrisdale 8428 73 Killarney 11799 70 Kitsilano 30441 59 Marpole 77012 61 Mount Pleasant 36105 48 Musqueam 572 19 Oakridge 9219 43 Renfrew-Collingwood 30892 88 Riley Park 14549 54 Shaughnessy 6289 54 South Cambie 5990 26 Stanley Park 4163 35 Strathcona 25565 40 Sunset 19599 63 Victoria-Fraserview 12263 61 West End 48389 28 West Point Grey 6721 47 . len(new_df[new_df[&#39;NEIGHBOURHOOD&#39;]==&#39;Central Business District&#39;].GEOHASH.value_counts()) . 49 . Categorzing crimes . Severe_crimes = [&#39;Vehicle Collision or Pedestrian Struck (with Fatality)&#39;, &#39;Homicide&#39;,&#39;Offence Against a Person&#39;,&#39;Vehicle Collision or Pedestrian Struck (with Injury)&#39;] Theft = [&#39;Theft from Vehicle&#39;,&#39;Other Theft&#39;,&#39;Theft of Vehicle&#39;,&#39;Theft of Bicycle&#39;] for idx,row in new_df.iterrows(): if str(row[&#39;TYPE&#39;]) in Severe_crimes: new_df.at[idx,&#39;CRIME_TYPE&#39;] = &#39;SEVERE&#39; elif str(row[&#39;TYPE&#39;]) in Theft: new_df.at[idx,&#39;CRIME_TYPE&#39;] = &#39;Theft&#39; elif str(row[&#39;TYPE&#39;]) == &#39;Mischief&#39;: new_df.at[idx,&#39;CRIME_TYPE&#39;] = &#39;Mischief&#39; else: new_df.at[idx,&#39;CRIME_TYPE&#39;] = &#39;B&amp;E&#39; new_df.head() . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y LAT LNG DATE WEEKDAY HOLIDAY CRIME_TYPE GEOHASH . 0 | Break and Enter Commercial | 2019 | 3 | 7 | 2 | 6 | 10XX SITKA SQ | Fairview | 490612.964805 | 5.457110e+06 | 49.266678 | -123.129029 | 2019-03-07 02:06:00 | 3 | 0 | B&amp;E | 5ed43e02 | . 1 | Break and Enter Commercial | 2019 | 8 | 27 | 4 | 12 | 10XX ALBERNI ST | West End | 491007.779775 | 5.459174e+06 | 49.285255 | -123.123649 | 2019-08-27 04:12:00 | 1 | 0 | B&amp;E | 5ed43e44 | . 2 | Break and Enter Commercial | 2014 | 8 | 8 | 5 | 13 | 10XX ALBERNI ST | West End | 491015.943352 | 5.459166e+06 | 49.285181 | -123.123536 | 2014-08-08 05:13:00 | 4 | 0 | B&amp;E | 5ed43e44 | . 3 | Break and Enter Commercial | 2005 | 11 | 14 | 3 | 9 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | 2005-11-14 03:09:00 | 0 | 0 | B&amp;E | 5ed43e44 | . 4 | Break and Enter Commercial | 2006 | 5 | 21 | 4 | 50 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 49.285132 | -123.123461 | 2006-05-21 04:50:00 | 6 | 0 | B&amp;E | 5ed43e44 | . new_df.sort_values(by=&#39;DATE&#39;,inplace=True) . def get_neighbourhood_data(data_frame,neighbourhood): neighbourhood_data = data_frame.groupby(&#39;NEIGHBOURHOOD&#39;) return neighbourhood_data.get_group(neighbourhood)[[&#39;CRIME_TYPE&#39;, &#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY&#39;, &#39;HOUR&#39;, &#39;MINUTE&#39; , &#39;WEEKDAY&#39;, &#39;HOLIDAY&#39;,&#39;GEOHASH&#39;]] . Get data from particular neighbourhood . neigh = &#39;West End&#39; neigh_df = get_neighbourhood_data(new_df,neigh) . neigh_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 48389 entries, 538267 to 109496 Data columns (total 9 columns): CRIME_TYPE 48389 non-null object YEAR 48389 non-null int64 MONTH 48389 non-null int64 DAY 48389 non-null int64 HOUR 48389 non-null int64 MINUTE 48389 non-null int64 WEEKDAY 48389 non-null int64 HOLIDAY 48389 non-null int64 GEOHASH 48389 non-null object dtypes: int64(7), object(2) memory usage: 3.7+ MB . from sklearn.preprocessing import LabelBinarizer # one hot encode crimetype field lb_crimetype = LabelBinarizer() neigh_df = neigh_df.join(pd.DataFrame(lb_crimetype.fit_transform(neigh_df[&#39;CRIME_TYPE&#39;]), columns=lb_crimetype.classes_, index=neigh_df.index)) . neigh_df.columns . Index([&#39;CRIME_TYPE&#39;, &#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY&#39;, &#39;HOUR&#39;, &#39;MINUTE&#39;, &#39;WEEKDAY&#39;, &#39;HOLIDAY&#39;, &#39;GEOHASH&#39;, &#39;B&amp;E&#39;, &#39;Mischief&#39;, &#39;SEVERE&#39;, &#39;Theft&#39;], dtype=&#39;object&#39;) . Split Train and Test . neigh_test = neigh_df[-5:] neigh_train = neigh_df[:-5] . neigh_train.shape . (48384, 13) . from sklearn.ensemble import RandomForestClassifier forest = RandomForestClassifier(min_samples_leaf=5) forest.fit(neigh_train.drop([&#39;GEOHASH&#39;,&#39;CRIME_TYPE&#39;],axis=1), neigh_train[&#39;GEOHASH&#39;]) . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=5, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . pos_pred = forest.predict(neigh_test.drop([&#39;GEOHASH&#39;,&#39;CRIME_TYPE&#39;],axis=1)) . pos_prob =forest.predict_proba(neigh_test.drop([&#39;GEOHASH&#39;,&#39;CRIME_TYPE&#39;],axis=1)) . for a,b in zip(pos_pred,neigh_test[&#39;GEOHASH&#39;]): print(a,b) . 5ed43e3e 5ed43e3e 5ed43e43 5ed43e4e 5ed43e4e 5ed43e3f 5ed43e43 5ed43e43 5ed43e3e 5ed43e3e . pos_prob.shape,forest.classes_ . ((5, 28), array([&#39;5ed43db0&#39;, &#39;5ed43db1&#39;, &#39;5ed43db2&#39;, &#39;5ed43db3&#39;, &#39;5ed43db7&#39;, &#39;5ed43dbc&#39;, &#39;5ed43dbd&#39;, &#39;5ed43dbe&#39;, &#39;5ed43dbf&#39;, &#39;5ed43dc0&#39;, &#39;5ed43dc1&#39;, &#39;5ed43dc3&#39;, &#39;5ed43dc4&#39;, &#39;5ed43e31&#39;, &#39;5ed43e38&#39;, &#39;5ed43e3a&#39;, &#39;5ed43e3b&#39;, &#39;5ed43e3c&#39;, &#39;5ed43e3d&#39;, &#39;5ed43e3e&#39;, &#39;5ed43e3f&#39;, &#39;5ed43e40&#39;, &#39;5ed43e41&#39;, &#39;5ed43e42&#39;, &#39;5ed43e43&#39;, &#39;5ed43e44&#39;, &#39;5ed43e4e&#39;, &#39;5ed43e4f&#39;], dtype=object)) . top_n = 3 pred_loc = [np.argsort(prob)[:top_n] for prob in pos_prob] . hashesh = forest.classes_ p = [[ghh.decode(hashesh[i],bits_per_char=4) for i in x] for x in pred_loc] p . [[(-123.13751220703125, 49.294281005859375), (-123.13201904296875, 49.275054931640625), (-123.12652587890625, 49.277801513671875)], [(-123.13751220703125, 49.294281005859375), (-123.13201904296875, 49.288787841796875), (-123.13201904296875, 49.277801513671875)], [(-123.13751220703125, 49.294281005859375), (-123.13201904296875, 49.288787841796875), (-123.13201904296875, 49.286041259765625)], [(-123.13751220703125, 49.294281005859375), (-123.13201904296875, 49.286041259765625), (-123.13201904296875, 49.275054931640625)], [(-123.13751220703125, 49.294281005859375), (-123.13201904296875, 49.275054931640625), (-123.14849853515625, 49.291534423828125)]] . map_van = folium.Map(location=[49.24, -123.11], zoom_start = 12,tiles=&#39;cartodbpositron&#39;) for pnt in pnt_arr: folium.Circle(location=[pnt[1], pnt[0]],radius=500,popup=&#39;Predicted&#39;,fill_color=&quot;#3db7e4&quot;).add_to(map_van) map_van.save(f&#39;predicted loaction.html&#39;) map_van .",
            "url": "https://isbhargav.github.io/portfolio/jupyter/matplotlib/sklearn/randomforest/knn/geohashing/feature-engineering/2020/11/01/Location_Prediction.html",
            "relUrl": "/jupyter/matplotlib/sklearn/randomforest/knn/geohashing/feature-engineering/2020/11/01/Location_Prediction.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "EDA on Vancouver City Crime dataset",
            "content": "import pandas as pd from datetime import datetime import matplotlib.pyplot as plt import seaborn as sns import plotly.graph_objects as go # Read the dataset df = pd.read_csv(&quot;crimedata_csv_all_years.csv&quot;) # Create a Date column df[&#39;DATE&#39;] = pd.to_datetime(df[[&#39;YEAR&#39;,&#39;MONTH&#39;,&#39;DAY&#39;]]) # Create a new feature &quot;Weekday&quot; df[&#39;WEEKDAY&#39;] = df[&#39;DATE&#39;].dt.dayofweek print(df.head()) . Get all the unique neighbourhoods in Vancouver City . print(df[&#39;NEIGHBOURHOOD&#39;].unique()) . [&#39;Oakridge&#39; &#39;Fairview&#39; &#39;West End&#39; &#39;Central Business District&#39; &#39;Hastings-Sunrise&#39; &#39;Strathcona&#39; &#39;Grandview-Woodland&#39; &#39;Kitsilano&#39; &#39;Kensington-Cedar Cottage&#39; &#39;Sunset&#39; &#39;Mount Pleasant&#39; &#39;Stanley Park&#39; &#39;Shaughnessy&#39; &#39;Marpole&#39; &#39;West Point Grey&#39; &#39;Victoria-Fraserview&#39; &#39;Kerrisdale&#39; &#39;Riley Park&#39; &#39;Arbutus Ridge&#39; &#39;Renfrew-Collingwood&#39; &#39;Killarney&#39; &#39;Dunbar-Southlands&#39; &#39;South Cambie&#39; &#39;Musqueam&#39; nan] . count_df = df[&#39;HOUR&#39;].value_counts() city_count = count_df[:,] plt.figure(figsize=(12,5)) sns.barplot(count_df.index, count_df.values, alpha=1) plt.title(&#39;Crime occurence at different time of the day&#39;) plt.ylabel(&#39;Number of Occurrences&#39;, fontsize=12) plt.xlabel(&#39;Hour&#39;, fontsize=12) plt.show() . for column in df: print(df[column].isna().value_counts()) . False 619574 Name: TYPE, dtype: int64 False 619574 Name: YEAR, dtype: int64 False 619574 Name: MONTH, dtype: int64 False 619574 Name: DAY, dtype: int64 False 619574 Name: HOUR, dtype: int64 False 619574 Name: MINUTE, dtype: int64 False 619561 True 13 Name: HUNDRED_BLOCK, dtype: int64 False 555000 True 64574 Name: NEIGHBOURHOOD, dtype: int64 False 619455 True 119 Name: X, dtype: int64 False 619455 True 119 Name: Y, dtype: int64 False 619574 Name: DATE, dtype: int64 False 619574 Name: WEEKDAY, dtype: int64 . Frequency of each crime type . new_df = df.groupby(&#39;YEAR&#39;) print(df[&#39;TYPE&#39;].value_counts()) . Theft from Vehicle 207712 Mischief 83355 Break and Enter Residential/Other 66065 Other Theft 64087 Offence Against a Person 61706 Theft of Vehicle 41394 Break and Enter Commercial 38645 Theft of Bicycle 30887 Vehicle Collision or Pedestrian Struck (with Injury) 25183 Vehicle Collision or Pedestrian Struck (with Fatality) 289 Homicide 251 Name: TYPE, dtype: int64 . nameplot = df[&#39;NEIGHBOURHOOD&#39;].value_counts().plot.bar(title=&#39;Count of each type of crime happened in Vancouver&#39;, figsize=(12,6)) nameplot.set_xlabel(&#39;category&#39;,size=20) nameplot.set_ylabel(&#39;crime count&#39;,size=20) . Text(0, 0.5, &#39;crime count&#39;) . Finding crime reported in each weekday . count_df = df[&#39;WEEKDAY&#39;].value_counts() city_count = count_df[:,] sns.barplot(count_df.index, count_df.values, alpha=1) plt.title(&#39;Crime occurence on different days of the week&#39;) plt.ylabel(&#39;Number of Occurrences&#39;, fontsize=12) plt.xlabel(&#39;Day of the week&#39;, fontsize=12) plt.show() . # year labels year_labels = sorted(df[&quot;YEAR&quot;].unique()) # crime types crime_types = sorted(df[&quot;TYPE&quot;].unique().tolist()) crime_count_by_year = pd.DataFrame(columns =[&quot;year&quot;]) crime_count_by_year[&quot;year&quot;] = year_labels crime_count_by_year for current_type in crime_types: current_crime = df[df[&quot;TYPE&quot;]==current_type] current_crime_counts = current_crime[&quot;YEAR&quot;].value_counts(sort=False) current_crime_index = current_crime_counts.index.tolist() current_crime_index, current_crime_counts = zip(*sorted(zip(current_crime_index, current_crime_counts))) crime_count_by_year[current_type] = current_crime_counts crime_count_by_year crime_types = sorted(df[&quot;TYPE&quot;].unique().tolist()) fig = go.Figure() for current_crime in crime_types: current_type_count = crime_count_by_year[current_crime] fig.add_trace( go.Scatter( x=year_labels, y=current_type_count, mode=&#39;lines+markers&#39;, name=current_crime ) ) fig.update_layout(title=&#39;Crimes Over the Years in Vancouver by Type&#39;, xaxis_title=&#39;Year&#39;, yaxis_title=&#39;Absolute Change&#39;, autosize=True, height=570 ) fig.update_layout(legend_orientation=&quot;h&quot;) fig.show() . new_df = df.groupby(&#39;YEAR&#39;) df2 = df[&#39;TYPE&#39;].value_counts() crime_types = df2.index new_df = df.groupby(&#39;TYPE&#39;) for value in crime_types: df1 = new_df.get_group(value) count_df = df1[&#39;YEAR&#39;].value_counts() plt.figure(figsize=(8.5,5)) sns.barplot(count_df.index, count_df.values, alpha=1) plt.title(value) plt.ylabel(&#39;Number of Occurrences&#39;, fontsize=12) plt.xlabel(&#39;Year&#39;, fontsize=12) plt.show() . Index([&#39;Theft from Vehicle&#39;, &#39;Mischief&#39;, &#39;Break and Enter Residential/Other&#39;, &#39;Other Theft&#39;, &#39;Offence Against a Person&#39;, &#39;Theft of Vehicle&#39;, &#39;Break and Enter Commercial&#39;, &#39;Theft of Bicycle&#39;, &#39;Vehicle Collision or Pedestrian Struck (with Injury)&#39;, &#39;Vehicle Collision or Pedestrian Struck (with Fatality)&#39;, &#39;Homicide&#39;], dtype=&#39;object&#39;) .",
            "url": "https://isbhargav.github.io/portfolio/jupyter/matplotlib/eda/2020/11/01/Exploratory_Analysis.html",
            "relUrl": "/jupyter/matplotlib/eda/2020/11/01/Exploratory_Analysis.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Predicting Crimes in Neighbourhood",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import holidays from datetime import datetime,date from sklearn.neighbors import KNeighborsClassifier import warnings from sklearn.ensemble import RandomForestClassifier from xgboost import XGBClassifier from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder warnings.filterwarnings(&quot;ignore&quot;) . df = pd.read_csv(&#39;crimedata_csv_all_years.csv&#39;) df.head() . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y . 0 | Break and Enter Commercial | 2012 | 12 | 14 | 8 | 52 | NaN | Oakridge | 491285.000000 | 5.453433e+06 | . 1 | Break and Enter Commercial | 2019 | 3 | 7 | 2 | 6 | 10XX SITKA SQ | Fairview | 490612.964805 | 5.457110e+06 | . 2 | Break and Enter Commercial | 2019 | 8 | 27 | 4 | 12 | 10XX ALBERNI ST | West End | 491007.779775 | 5.459174e+06 | . 3 | Break and Enter Commercial | 2014 | 8 | 8 | 5 | 13 | 10XX ALBERNI ST | West End | 491015.943352 | 5.459166e+06 | . 4 | Break and Enter Commercial | 2005 | 11 | 14 | 3 | 9 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | . Number of Missing data in each column . df.isnull().sum() . TYPE 0 YEAR 0 MONTH 0 DAY 0 HOUR 0 MINUTE 0 HUNDRED_BLOCK 13 NEIGHBOURHOOD 64574 X 119 Y 119 dtype: int64 . df[&#39;Date&#39;] = pd.to_datetime(df[[&#39;YEAR&#39;,&#39;MONTH&#39;,&#39;DAY&#39;,&#39;HOUR&#39;,&#39;MINUTE&#39;]]) df[&#39;WEEKDAY&#39;] = df[&#39;Date&#39;].dt.dayofweek ca_holidays = holidays.Canada(prov=&#39;BC&#39;) for i,row in df.iterrows(): if df.iloc[i][&#39;Date&#39;] in ca_holidays: df.at[i,&#39;HOLIDAY&#39;] = 1 else: df.at[i,&#39;HOLIDAY&#39;] = 0 df.head() . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y Date WEEKDAY HOLIDAY . 0 | Break and Enter Commercial | 2012 | 12 | 14 | 8 | 52 | NaN | Oakridge | 491285.000000 | 5.453433e+06 | 2012-12-14 08:52:00 | 4 | 0.0 | . 1 | Break and Enter Commercial | 2019 | 3 | 7 | 2 | 6 | 10XX SITKA SQ | Fairview | 490612.964805 | 5.457110e+06 | 2019-03-07 02:06:00 | 3 | 0.0 | . 2 | Break and Enter Commercial | 2019 | 8 | 27 | 4 | 12 | 10XX ALBERNI ST | West End | 491007.779775 | 5.459174e+06 | 2019-08-27 04:12:00 | 1 | 0.0 | . 3 | Break and Enter Commercial | 2014 | 8 | 8 | 5 | 13 | 10XX ALBERNI ST | West End | 491015.943352 | 5.459166e+06 | 2014-08-08 05:13:00 | 4 | 0.0 | . 4 | Break and Enter Commercial | 2005 | 11 | 14 | 3 | 9 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 2005-11-14 03:09:00 | 0 | 0.0 | . Severe_crimes = [&#39;Vehicle Collision or Pedestrian Struck (with Fatality)&#39;, &#39;Homicide&#39;,&#39;Offence Against a Person&#39;,&#39;Vehicle Collision or Pedestrian Struck (with Injury)&#39;] Theft = [&#39;Theft from Vehicle&#39;,&#39;Other Theft&#39;,&#39;Theft of Vehicle&#39;,&#39;Theft of Bicycle&#39;] for idx,row in df.iterrows(): if str(row[&#39;TYPE&#39;]) in Severe_crimes: df.at[idx,&#39;CRIME_TYPE&#39;] = &#39;SEVERE&#39; elif str(row[&#39;TYPE&#39;]) in Theft: df.at[idx,&#39;CRIME_TYPE&#39;] = &#39;Theft&#39; elif str(row[&#39;TYPE&#39;]) == &#39;Mischief&#39;: df.at[idx,&#39;CRIME_TYPE&#39;] = &#39;Mischief&#39; else: df.at[idx,&#39;CRIME_TYPE&#39;] = &#39;B&amp;E&#39; df.head(5) df.HOLIDAY.value_counts() . 0.0 600443 1.0 19131 Name: HOLIDAY, dtype: int64 . %matplotlib inline sns.countplot(x=&#39;YEAR&#39;,data=df[[&#39;Y&#39;,&#39;X&#39;,&#39;YEAR&#39;]][df.X.isnull()]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1e7f0fc8f48&gt; . This shows that most of the missing coordinate values are in year 2003 (almost half) . Let&#39;s drop rows with missing cordinates . df.dropna(subset=[&#39;X&#39;,&#39;Y&#39;],inplace=True) df.head() . TYPE YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD X Y Date WEEKDAY HOLIDAY CRIME_TYPE . 0 | Break and Enter Commercial | 2012 | 12 | 14 | 8 | 52 | NaN | Oakridge | 491285.000000 | 5.453433e+06 | 2012-12-14 08:52:00 | 4 | 0.0 | B&amp;E | . 1 | Break and Enter Commercial | 2019 | 3 | 7 | 2 | 6 | 10XX SITKA SQ | Fairview | 490612.964805 | 5.457110e+06 | 2019-03-07 02:06:00 | 3 | 0.0 | B&amp;E | . 2 | Break and Enter Commercial | 2019 | 8 | 27 | 4 | 12 | 10XX ALBERNI ST | West End | 491007.779775 | 5.459174e+06 | 2019-08-27 04:12:00 | 1 | 0.0 | B&amp;E | . 3 | Break and Enter Commercial | 2014 | 8 | 8 | 5 | 13 | 10XX ALBERNI ST | West End | 491015.943352 | 5.459166e+06 | 2014-08-08 05:13:00 | 4 | 0.0 | B&amp;E | . 4 | Break and Enter Commercial | 2005 | 11 | 14 | 3 | 9 | 10XX ALBERNI ST | West End | 491021.385727 | 5.459161e+06 | 2005-11-14 03:09:00 | 0 | 0.0 | B&amp;E | . Now let us see different neighbourhood . df.NEIGHBOURHOOD.value_counts(),len(df.NEIGHBOURHOOD.value_counts()) . (Central Business District 136148 West End 48389 Fairview 36501 Mount Pleasant 36104 Grandview-Woodland 31413 Renfrew-Collingwood 30892 Kitsilano 30442 Kensington-Cedar Cottage 28232 Strathcona 25567 Hastings-Sunrise 21124 Sunset 19600 Marpole 15054 Riley Park 14550 Victoria-Fraserview 12264 Killarney 11798 Oakridge 9221 Dunbar-Southlands 8752 Kerrisdale 8428 Arbutus Ridge 6790 West Point Grey 6719 Shaughnessy 6289 South Cambie 5990 Stanley Park 4163 Musqueam 570 Name: NEIGHBOURHOOD, dtype: int64, 24) . There are 24 Neighbourhood values . Lets see relation between X,Y and Neighourhood . plt.figure(figsize=(20,20)) g =sns.scatterplot(x=&quot;X&quot;, y=&quot;Y&quot;, hue=&quot;NEIGHBOURHOOD&quot;, data=df,legend=False); g.set(xscale=&quot;log&quot;); . There seems to be very high corelation between coordinate value and the neighbourhood and it makes sense because coordindies in a way represents location of neighbourhood. There we can use KNN to impute the missing values of neighbourhood in our data. . Checking if cordinates are missing in missing neighbourhood instances . cols = [&#39;X&#39;,&#39;Y&#39;] df[cols][df[&#39;NEIGHBOURHOOD&#39;].isnull()].isnull().sum() . X 0 Y 0 dtype: int64 . Therefore no coordinate values are missing in case of missing neighbourhood innstances . df.NEIGHBOURHOOD.unique() . array([&#39;Oakridge&#39;, &#39;Fairview&#39;, &#39;West End&#39;, &#39;Central Business District&#39;, &#39;Hastings-Sunrise&#39;, &#39;Strathcona&#39;, &#39;Grandview-Woodland&#39;, &#39;Kitsilano&#39;, &#39;Kensington-Cedar Cottage&#39;, &#39;Sunset&#39;, &#39;Mount Pleasant&#39;, &#39;Stanley Park&#39;, &#39;Shaughnessy&#39;, &#39;Marpole&#39;, &#39;West Point Grey&#39;, &#39;Victoria-Fraserview&#39;, &#39;Kerrisdale&#39;, &#39;Riley Park&#39;, &#39;Arbutus Ridge&#39;, &#39;Renfrew-Collingwood&#39;, &#39;Killarney&#39;, &#39;Dunbar-Southlands&#39;, &#39;South Cambie&#39;, &#39;Musqueam&#39;, nan], dtype=object) . df.dropna(subset=[&#39;HUNDRED_BLOCK&#39;],inplace=True) . train_df = df.dropna(subset=[&#39;NEIGHBOURHOOD&#39;],inplace=False) x_train = train_df[[&#39;X&#39;,&#39;Y&#39;]] y_train = train_df[&#39;NEIGHBOURHOOD&#39;] test_df=df[df[&#39;NEIGHBOURHOOD&#39;].isnull()] classifier = KNeighborsClassifier(n_neighbors=5,metric=&#39;haversine&#39;) classifier.fit(x_train, y_train) y_pred = classifier.predict(test_df[[&#39;X&#39;,&#39;Y&#39;]]) test_df[&#39;NEIGHBOURHOOD&#39;] = y_pred . new_df = df[0:0] new_df=pd.concat([new_df,train_df],ignore_index=True) new_df=pd.concat([new_df,test_df],ignore_index=True) print(new_df.isnull().any().sum()) new_df[&#39;Date&#39;] = pd.to_datetime(new_df[[&#39;YEAR&#39;,&#39;MONTH&#39;,&#39;DAY&#39;,&#39;HOUR&#39;,&#39;MINUTE&#39;]]) new_df.set_index(&#39;Date&#39;,inplace=True) new_df.drop(&#39;TYPE&#39;,axis=1,inplace=True) print(new_df.head()) . 0 YEAR MONTH DAY HOUR MINUTE HUNDRED_BLOCK Date 2019-03-07 02:06:00 2019 3 7 2 6 10XX SITKA SQ 2019-08-27 04:12:00 2019 8 27 4 12 10XX ALBERNI ST 2014-08-08 05:13:00 2014 8 8 5 13 10XX ALBERNI ST 2005-11-14 03:09:00 2005 11 14 3 9 10XX ALBERNI ST 2006-05-21 04:50:00 2006 5 21 4 50 10XX ALBERNI ST NEIGHBOURHOOD X Y WEEKDAY Date 2019-03-07 02:06:00 Fairview 490612.964805 5.457110e+06 3 2019-08-27 04:12:00 West End 491007.779775 5.459174e+06 1 2014-08-08 05:13:00 West End 491015.943352 5.459166e+06 4 2005-11-14 03:09:00 West End 491021.385727 5.459161e+06 0 2006-05-21 04:50:00 West End 491021.385727 5.459161e+06 6 HOLIDAY CRIME_TYPE Date 2019-03-07 02:06:00 0.0 B&amp;E 2019-08-27 04:12:00 0.0 B&amp;E 2014-08-08 05:13:00 0.0 B&amp;E 2005-11-14 03:09:00 0.0 B&amp;E 2006-05-21 04:50:00 0.0 B&amp;E . new_df.isnull().sum() . YEAR 0 MONTH 0 DAY 0 HOUR 0 MINUTE 0 HUNDRED_BLOCK 0 NEIGHBOURHOOD 0 X 0 Y 0 WEEKDAY 0 HOLIDAY 0 CRIME_TYPE 0 dtype: int64 . new_df.sort_values([&#39;Date&#39;],axis=0,ascending=True,inplace=True) # List of years where Crime_Type is to be predicted year_predict = [2015,2016,2017,2018,2019] # Initialize the Random Forest Classifier Model rfc = RandomForestClassifier() # Encode the Categorical Features label_encoder = LabelEncoder() new_df[&#39;NEIGHBOURHOOD&#39;] = label_encoder.fit_transform(new_df[&#39;NEIGHBOURHOOD&#39;]) # Create a different model for predicting crime for each year for i in year_predict: # Create training and testing dataset for each Iteration mask = (new_df[&#39;YEAR&#39;] &gt;= 2003) &amp; (new_df[&#39;YEAR&#39;] &lt; i) train_x = new_df.loc[mask] train_y = train_x.CRIME_TYPE train_x = train_x[[ &#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY&#39;, &#39;HOUR&#39;,&#39;WEEKDAY&#39;, &#39;NEIGHBOURHOOD&#39;,&#39;HOLIDAY&#39;]] new_mask = (new_df[&#39;YEAR&#39;] == i) test_x = new_df.loc[new_mask] test_y = test_x.CRIME_TYPE test_x = test_x[[ &#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY&#39;, &#39;HOUR&#39;,&#39;WEEKDAY&#39;, &#39;NEIGHBOURHOOD&#39;,&#39;HOLIDAY&#39;]] print(&quot;Year: &quot;,i) rfc.fit(train_x.values,train_y.values) y_pred = rfc.predict(test_x) print(&quot;Accuracy: &quot;,accuracy_score(test_y,y_pred)) . Year: 2015 Accuracy: 0.5991150957675963 Year: 2016 Accuracy: 0.6260435379900666 Year: 2017 Accuracy: 0.6284370184118745 Year: 2018 Accuracy: 0.6302092907224075 Year: 2019 Accuracy: 0.6614870243591785 . model = XGBClassifier() # Create a different model for predicting crime for each year for i in year_predict: # Create training and testing dataset for each Iteration mask = (new_df[&#39;YEAR&#39;] &gt;= 2003) &amp; (new_df[&#39;YEAR&#39;] &lt; i) train_x = new_df.loc[mask] train_y = train_x.CRIME_TYPE train_x = train_x[[ &#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY&#39;, &#39;HOUR&#39;,&#39;WEEKDAY&#39;, &#39;NEIGHBOURHOOD&#39;,&#39;HOLIDAY&#39;]] new_mask = (new_df[&#39;YEAR&#39;] == i) test_x = new_df.loc[new_mask] test_y = test_x.CRIME_TYPE test_x = test_x[[ &#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY&#39;, &#39;HOUR&#39;,&#39;WEEKDAY&#39;, &#39;NEIGHBOURHOOD&#39;,&#39;HOLIDAY&#39;]] print(&quot;Year: &quot;,i) model.fit(train_x, train_y) y_pred_xgb = model.predict(test_x) print(&quot;Accuracy: &quot;,accuracy_score(test_y,y_pred_xgb)) . Year: 2015 Accuracy: 0.6681318041567212 Year: 2016 Accuracy: 0.6848251083166015 Year: 2017 Accuracy: 0.6859166734257982 Year: 2018 Accuracy: 0.6932853654053203 Year: 2019 Accuracy: 0.7254258875975164 .",
            "url": "https://isbhargav.github.io/portfolio/jupyter/matplotlib/sklearn/randomforest/knn/xgboost/2020/11/01/Crime_Prediction.html",
            "relUrl": "/jupyter/matplotlib/sklearn/randomforest/knn/xgboost/2020/11/01/Crime_Prediction.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Image Classification on SUN397",
            "content": "About dataset . The dataset contains 108,753 images of 397 categories, used in the Scene UNderstanding (SUN) benchmark. The number of images varies across categories, but there are at least 100 images per category. . link:Dataset . Task . Our task is to create build a DCNN model which is able to classify images among 397 different categories. . Approch . We will use transfer learning with few tricks to train our DCNN model which will be able to produce high accuracy on our task. To be specific we will use the following techniques . Transfer Learning | Data Augmentation | Feature Fusion Technique | Discriminative Learning Rate | One-cycle Policy | . # # by Bolei Zhou # last modified by Bolei Zhou, Dec.27, 2017 with latest pytorch and torchvision (upgrade your torchvision please if there is trn.Resize error) import torch from torch.autograd import Variable as V import torchvision.models as models from torchvision import transforms as trn from torch.nn import functional as F import os from PIL import Image from fastai import * from fastai.vision import * from fastai.callbacks import * # th architecture to use arch = &#39;resnet50&#39; . Downloading Pretrained Model . In almost every deep learning project it is always better you start with a pretrained model. But you should also be carefull when selecting a pretrained model as you want to choose something that is trained on dataset which is similar to yours. The reason we always want to start with any pretrained model is if you want to train a classifier, any classifier, the initial layers are going to detect slant lines no matter what you want to classify. Hence, it does not make sense to train them every time you create a neural network. It is only the final layers of our network, the layers that learn to identify classes specific to your project that need training. This is also known as Transfer learning . So we start with a ResNet50 Model which is trained on Places 365 dataset. Places 365 dataset is also a scene centric dataset which is similar to ours in this case hence it makes sense to use a pretrained model of places 365 dataset. . model_file = &#39;%s_places365.pth.tar&#39; % arch if not os.access(model_file, os.W_OK): weight_url = &#39;http://places2.csail.mit.edu/models_places365/&#39; + model_file os.system(&#39;wget &#39; + weight_url) . model = models.__dict__[arch](num_classes=365) checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage) state_dict = {str.replace(k,&#39;module.&#39;,&#39;&#39;): v for k,v in checkpoint[&#39;state_dict&#39;].items()} model.load_state_dict(state_dict) . &lt;All keys matched successfully&gt; . Definng Custom Layers to be used . We will only use the initial layers from our pretrained models as the last layers are specific for the task, hence they are of no use to us. So we will only use the convolutional layers from our pretrained models and replace the linear layers with ours. . Feature Fussion . We will merge features from Average pooling and Max pooling, and then we will feed it into our classification layers. This hybrid approch is known to perform better that using any single pooling layer as we have more variety of features from our convolutional layers. . def init_weights(m): if type(m) == nn.Linear: nn.init.kaiming_normal_(m.weight) m.bias.data.fill_(0.01) # Apply AveragePooling and MaxPooling and conat them class AdaptiveConcatPool2d(nn.Module): def __init__(self, sz=None): super().__init__() sz = sz or (1,1) self.ap = nn.AdaptiveAvgPool2d(sz) self.mp = nn.AdaptiveMaxPool2d(sz) def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1) # Apply function on layer class Lambda(nn.Module): def __init__(self, f): super().__init__(); self.f=f def forward(self, x): return self.f(x) # Flatten Layer class Flatten(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.view(x.size(0), -1) . Defining the Model . We declare our model with new layers . class SUNClassifier(nn.Module): def __init__(self,model): super(SUNClassifier,self).__init__() # Cut the pretrained model before classification layers to get deep features head = [m for m in model.children() if not (isinstance(m,nn.AdaptiveAvgPool2d) or isinstance(m,nn.Linear))] self.head = nn.Sequential(*head) # Apply Max Pool and Avg Pool and concat them self.pooling = nn.Sequential(AdaptiveConcatPool2d(),Flatten()) # Defing Classification Layers self.tail = nn.Sequential( nn.BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), nn.Dropout(p=0.25, inplace=False), nn.Linear(in_features=4096, out_features=512, bias=True), nn.ReLU(inplace=True), nn.BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), nn.Dropout(p=0.5, inplace=False), nn.Linear(in_features=512, out_features=397, bias=True), ) # Initialze Classification Layers with Kaiming Normal self.tail.apply(init_weights) def forward(self, x): x = self.head(x) x = self.pooling(x) x = self.tail(x) return x . Initialize the Model . mm = SUNClassifier(model) . mm . SUNClassifier( (head): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (5): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (6): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (7): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) ) (pooling): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=(1, 1)) (mp): AdaptiveMaxPool2d(output_size=(1, 1)) ) (1): Flatten() ) (tail): Sequential( (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): Dropout(p=0.25, inplace=False) (2): Linear(in_features=4096, out_features=512, bias=True) (3): ReLU(inplace=True) (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=512, out_features=397, bias=True) ) ) . Loading Dataset . For the results of this experiment we have chosen at random one of the available ten training and testing partitions provided by the authors of the dataset. The randomly chosen partition is the fifth partition which has two files namely: Training_05.txt and Testing_05.txt. Each of these files contains 19850(50x397) image paths where each class has fifty images belonging to it. . path = Path(&#39;SUN397&#39;) . train_df = pd.read_csv(&#39;partations/Training_05.txt&#39;,names=[&#39;img&#39;] ) test_df = pd.read_csv(&#39;partations/Testing_05.txt&#39;,names=[&#39;img&#39;] ) . train_df.shape,test_df.shape . ((19850, 1), (19850, 1)) . train_df.head() . img . 0 /a/abbey/sun_aqkoegfgyqvketwj.jpg | . 1 /a/abbey/sun_aowgddbfwuycbznu.jpg | . 2 /a/abbey/sun_askedxjewutmkwey.jpg | . 3 /a/abbey/sun_atqevsqpwuskzqje.jpg | . 4 /a/abbey/sun_aemytrbhywxpeety.jpg | . Getting Labels . The labels of the images are the name of the folder it is in. We use regex to extract the labels from its path . train_labels = train_df.img.str.extract(&#39;/./([/ w]+)/ w+&#39;) test_labels = test_df.img.str.extract(&#39;/./([/ w]+)/ w+&#39;) . train_df[&#39;label&#39;] = train_labels train_df[&#39;is_valid&#39;] = False test_df[&#39;label&#39;] = test_labels test_df[&#39;is_valid&#39;] = True . df = pd.concat([train_df,test_df]) df.head() . img label is_valid . 0 /a/abbey/sun_aqkoegfgyqvketwj.jpg | abbey | False | . 1 /a/abbey/sun_aowgddbfwuycbznu.jpg | abbey | False | . 2 /a/abbey/sun_askedxjewutmkwey.jpg | abbey | False | . 3 /a/abbey/sun_atqevsqpwuskzqje.jpg | abbey | False | . 4 /a/abbey/sun_aemytrbhywxpeety.jpg | abbey | False | . Image Augmentation . Image data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize . We will use following augmentations.Each image undergoes different augmentation methods during training. . Rotation: Degree range for random rotations(-10, +10). | Symmetric Warp: Change the angle at which image is viewed.(-0.2, +0.2) | Zoom range: Range for random zoom(1, 1.1). | Left Right flip: Randomly flip inputs left or right. | Brightness: Changes brightness of Images(0.4, 0.6). | Contrast: Changes contrast of Images(0.8, 1.25). | Crop and Pad: Randomly crops and pads the remaining part of image. | tfms = get_transforms() tfms . ([RandTransform(tfm=TfmCrop (crop_pad), kwargs={&#39;row_pct&#39;: (0, 1), &#39;col_pct&#39;: (0, 1), &#39;padding_mode&#39;: &#39;reflection&#39;}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True), RandTransform(tfm=TfmPixel (flip_lr), kwargs={}, p=0.5, resolved={}, do_run=True, is_random=True, use_on_y=True), RandTransform(tfm=TfmCoord (symmetric_warp), kwargs={&#39;magnitude&#39;: (-0.2, 0.2)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True), RandTransform(tfm=TfmAffine (rotate), kwargs={&#39;degrees&#39;: (-10.0, 10.0)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True), RandTransform(tfm=TfmAffine (zoom), kwargs={&#39;scale&#39;: (1.0, 1.1), &#39;row_pct&#39;: (0, 1), &#39;col_pct&#39;: (0, 1)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True), RandTransform(tfm=TfmLighting (brightness), kwargs={&#39;change&#39;: (0.4, 0.6)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True), RandTransform(tfm=TfmLighting (contrast), kwargs={&#39;scale&#39;: (0.8, 1.25)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True)], [RandTransform(tfm=TfmCrop (crop_pad), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True)]) . data = (ImageList.from_df(df,path).split_from_df().label_from_df().transform(tfms,size=224) .databunch(bs=128) .normalize(imagenet_stats)) . data . ImageDataBunch; Train: LabelList (19850 items) x: ImageList Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224) y: CategoryList abbey,abbey,abbey,abbey,abbey Path: SUN397; Valid: LabelList (19850 items) x: ImageList Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224) y: CategoryList abbey,abbey,abbey,abbey,abbey Path: SUN397; Test: None . data.train_dl.c,data.valid_dl.c . (397, 397) . data.show_batch(rows=3, figsize=(5,5)) . Compile Our model with data and metrics . learner = Learner(data,mm,metrics=[accuracy,error_rate],callback_fns=[partial(CSVLogger, append=True)]) . Divide the model into layer goups to use discriminative learning rates . The Intutuion behind to use discriminative learning rates is in the pretrained model, the layers closer to the input are more likely to have learned more general features. Thus, we don’t want to change them much. For these early layers, we set the LR to be very low. We increase the LR per layer gradually as we move deeper into the model. . learner.layer_groups = [learner.layer_groups[0][:57],learner.layer_groups[0][57:124],learner.layer_groups[0][124:]] . Freeze the pretrained Weights . First we will only train our final layers which were added by us. We will freeze the layers from our pretrained model. The intution here is our pretrained models has already learned some features where as our new layers haven&#39;t learned anything yet. Hence, we start by training our new layers only. You can think as we are trying to syncing our two parts of our model . learner.freeze() . Finding optimal learning rate . learner.lr_find() . LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . learner.recorder.plot() . Train the model using one cycle policy . One Cycle Policy was discoverd by Leslie Smith. This allows us to train our model at higher learning rate with regularization. To read more about it refer to https://fastai1.fast.ai/callbacks.one_cycle.html . learner.fit_one_cycle(5,3e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 3.053918 | 1.602363 | 0.572796 | 0.427204 | 07:32 | . 1 | 2.055854 | 1.556024 | 0.587355 | 0.412645 | 07:34 | . 2 | 1.587337 | 1.293446 | 0.640353 | 0.359647 | 07:34 | . 3 | 1.196448 | 1.118837 | 0.680453 | 0.319547 | 07:39 | . 4 | 0.943023 | 1.081918 | 0.692040 | 0.307960 | 07:35 | . Unfreeze the complete model . Now we will train our entire model. . learner.unfreeze() . learner.lr_find() . LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . learner.recorder.plot() . learner.fit_one_cycle(3, max_lr=slice(1e-6,1e-5)) . epoch train_loss valid_loss accuracy error_rate time . 0 | 0.865293 | 1.080561 | 0.691839 | 0.308161 | 07:41 | . 1 | 0.841334 | 1.074188 | 0.693401 | 0.306599 | 07:40 | . 2 | 0.835416 | 1.072479 | 0.693098 | 0.306902 | 07:39 | . learner.freeze() . Save the Pytorch Model Weight . torch.save(learner.model.state_dict(), &#39;Resnet50-places-run2.pt&#39;) . learner.save(&#39;resnet50-place-run2&#39;) . learner.load(&#39;resnet50-place-run2&#39;) . Learner(data=ImageDataBunch; Train: LabelList (19850 items) x: ImageList Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224) y: CategoryList abbey,abbey,abbey,abbey,abbey Path: SUN397; Valid: LabelList (19850 items) x: ImageList Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224) y: CategoryList abbey,abbey,abbey,abbey,abbey Path: SUN397; Test: None, model=SUNClassifier( (head): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (5): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (6): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (7): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) ) (pooling): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=(1, 1)) (mp): AdaptiveMaxPool2d(output_size=(1, 1)) ) (1): Flatten() ) (tail): Sequential( (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): Dropout(p=0.25, inplace=False) (2): Linear(in_features=4096, out_features=512, bias=True) (3): ReLU(inplace=True) (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=512, out_features=397, bias=True) ) ), opt_func=functools.partial(&lt;class &#39;torch.optim.adam.Adam&#39;&gt;, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[&lt;function accuracy at 0x7f9fd2124320&gt;, &lt;function error_rate at 0x7f9fc814c560&gt;], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath(&#39;SUN397&#39;), model_dir=&#39;models&#39;, callback_fns=[functools.partial(&lt;class &#39;fastai.basic_train.Recorder&#39;&gt;, add_time=True, silent=False), functools.partial(&lt;class &#39;fastai.callbacks.csv_logger.CSVLogger&#39;&gt;, append=True)], callbacks=[], layer_groups=[Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (8): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (10): ReLU(inplace=True) (11): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (13): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (17): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (19): ReLU(inplace=True) (20): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (22): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (23): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (24): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (26): ReLU(inplace=True) (27): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (29): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (30): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (31): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (32): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (33): ReLU(inplace=True) (34): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (36): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (38): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (40): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (42): ReLU(inplace=True) (43): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (44): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (45): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (46): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (47): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (49): ReLU(inplace=True) (50): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (51): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (52): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (53): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (54): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (55): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (56): ReLU(inplace=True) ), Sequential( (57): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (58): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (59): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (60): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (61): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (62): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (63): ReLU(inplace=True) (64): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (65): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (66): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (67): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (68): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (69): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (70): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (71): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (72): ReLU(inplace=True) (73): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (74): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (75): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (76): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (77): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (78): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (79): ReLU(inplace=True) (80): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (81): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (82): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (83): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (84): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (85): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (86): ReLU(inplace=True) (87): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (88): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (89): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (90): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (91): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (92): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (93): ReLU(inplace=True) (94): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (95): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (96): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (97): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (98): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (99): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (100): ReLU(inplace=True) (101): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (102): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (103): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (104): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (105): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (106): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (107): ReLU(inplace=True) (108): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (109): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (110): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (111): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (112): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (113): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (114): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (115): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (116): ReLU(inplace=True) (117): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (118): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (119): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (120): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (121): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (122): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (123): ReLU(inplace=True) ), Sequential( (124): AdaptiveAvgPool2d(output_size=(1, 1)) (125): AdaptiveMaxPool2d(output_size=(1, 1)) (126): Flatten() (127): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (128): Dropout(p=0.25, inplace=False) (129): Linear(in_features=4096, out_features=512, bias=True) (130): ReLU(inplace=True) (131): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (132): Dropout(p=0.5, inplace=False) (133): Linear(in_features=512, out_features=397, bias=True) )], add_time=True, silent=False) . learner.validate(metrics=[accuracy]) . [1.0724795, tensor(0.6931)] . Final Accuray on test set . 69.31% .",
            "url": "https://isbhargav.github.io/portfolio/jupyter/pytorch/fastai/transfer-learning/discriminative-learning-rate/one-cycle-policy/2020/03/12/Image-calssification-sun397.html",
            "relUrl": "/jupyter/pytorch/fastai/transfer-learning/discriminative-learning-rate/one-cycle-policy/2020/03/12/Image-calssification-sun397.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://isbhargav.github.io/portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://isbhargav.github.io/portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About Me . Hi, I’m currently a graduate student at Lakehead University, Thunder Bay. My coursework is mostly realted to Machine Learning and Deep Learning.However, I’m always curious! I love to learn in general especially a new stack/technology. My focus right now is to learn the most I can in terms of different technologies and specialize later! . I love cs, so I do cs . I am really interested in: . programming competitions technology algorithms &amp; data structures AI &amp; machine learning . I am just an enthusiast, but a really motivated one :D . Feel free to message me about anything related to tech, I’m always open to a conversation! .",
          "url": "https://isbhargav.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://isbhargav.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}